<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spring-boot集成RabbitMQ]]></title>
    <url>%2F2018%2F07%2F19%2Fspring-boot%E9%9B%86%E6%88%90RabbitMQ%2F</url>
    <content type="text"><![CDATA[前言消息队列是日常开发中非常常用的工具，当调用方不需要关心执行结果时，你可以使用它来解耦。本文简单介绍下spring-boot中快速使用RabbitMQ完成队列的发送与消费。 AMQPAMQP协议AMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议,是应用层协议的一个开放标准,为面向消息的中间件设计。 消息代理消息代理（message brokers）从发布者（publishers）亦称生产者（producers）那儿接收消息，并根据既定的路由规则把接收到的消息发送给处理消息的消费者（consumers）。 由于AMQP是一个网络协议，所以这个过程中的发布者，消费者，消息代理 可以存在于不同的设备上。 AMQP协议模型这里推荐直接查看 AMQP 0-9-1 模型解析。 RabbitMQ与AMQP的关系RabbitMQ采用Erlang语言开发。是AMQP协议的一个实现。 安装rabbitMQ根据RabbitMQ文档 Installing on Debian and Ubuntu进行安装。 Windows安装一. 下载并安装Erlang。官网下载页：http://www.erlang.org/downloads。 二. 下载并安装RabbitMQ。官网下载页：https://www.rabbitmq.com/download.html。安装完成后会自动注册到服务中启动。 Mac OS X安装一. 安装Erlang，执行命令：brew install erlang。 二. 安装RabbitMQ，执行命令：brew install rabbitmq。 Ubuntu安装一. 安装Erlang，执行命令：apt-get install erlang erlang-nox。 二. 在系统中加入RabbitMQ apt 仓库: echo &#39;deb http://www.rabbitmq.com/debian/ testing main&#39; | sudo tee /etc/apt/sources.list.d/rabbitmq.list 三. 添加signing key，执行命令: wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -。 四. 更新APT仓库的package list，执行命令:apt-get update 五. 安装Rabbit Server，执行命令：apt-get install rabbitmq-server。 web控制台RabbitMQ内置提供了Web管理插件。 开启RabbitMQ web管理 Windows系统：进入插件目录:D:\rabbitmq\rabbitmq_server-3.7.7\sbin，运行命令：rabbitmq-plugins.bat enable rabbitmq_management Linux和Mac系统: 运行命令：rabbitmq-plugins enable rabbitmq_management。 通过浏览器访问web管理界面：http://localhost:15672/，默认登录用户:guest,密码:guest。 spring-boot中使用RabbitMQ三步完成集成: maven依赖 配置RabbitMQ 代码使用及测试 maven依赖仅需添加 &lt;!-- amqp,用于支持RabbitMQ --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; 配置rabbitMQ配置非常简单，在application.yml添加rabbitMQ相关: # rabbitMQ spring: rabbitmq: host: 127.0.0.1 port: 5672 username: guest password: guest 代码中使用及测试一. 在配置类中创建一个hello队列： @Configuration public class MessageQueueConfig { @Bean public org.springframework.amqp.core.Queue helloQueueByRabbitMQ() { return new org.springframework.amqp.core.Queue(&quot;hello&quot;); } } 二. 消息推送端，向hello队列发送一条消息: @Component @Slf4j public class HelloRabbitMQSender { @Autowired private AmqpTemplate rabbitTemplate; public void send(String content) { log.info(&quot;RabbitMQ send : {}&quot;, content); this.rabbitTemplate.convertAndSend(&quot;hello&quot;, content); } } 三. 消息消费端，消费一条hello队列里的消息: @Component @RabbitListener(queues = &quot;hello&quot;) @Slf4j public class HelloRabbitMQConsumer { @RabbitHandler public void process(String hello) { log.info(&quot;RabbitMQ receiver : {}&quot;, hello); } } 四. 单元测试发送一条消息: @RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest(classes = App.class) public class HelloRabbitMQSenderTest { @Autowired private HelloRabbitMQSender sender; @Test public void sendHello() { sender.send(&quot;Hello &quot; + new Date()); } } 输出结果，可以确认消息发送并且被消费了： 2018-07-16 14:23:09.844 [main] INFO com.crw.mq.HelloRabbitMQSender - RabbitMQ send : Hello Mon Jul 16 14:23:09 CST 2018 2018-07-16 14:23:09.872 [SimpleAsyncTaskExecutor-1] INFO com.crw.mq.HelloRabbitMQConsumer - RabbitMQ receiver : Hello Mon Jul 16 14:23:09 CST 2018 这里仅仅简单介绍了使用RabbitMQ基于队列完成简单的点对点的使用，这也是在web开发中最常用的方式(易于系统解耦、消峰)。更多的使用还是参考官方文档。]]></content>
      <categories>
        <category>spring-boot</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis集成分页插件pageHelper]]></title>
    <url>%2F2018%2F07%2F14%2Fmybatis%E9%9B%86%E6%88%90%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6pageHelper%2F</url>
    <content type="text"><![CDATA[前言web开发少不了分页，本文将简单介绍使用基于spring-boot + mybatis + pageHelper技术实现数据库分页。 本文算是对上一篇spring-boot集成mybatis的一个补充。 pageHelper简介使用MyBatis作为数据访问层进行对sql语句分页的最好用的插件。 建议去pageHelper官网看一看。 特点： 物理分页：支持常见的12种数据库。Oracle,MySql,MariaDB,SQLite,DB2,PostgreSQL,SqlServer等。 支持多种分页方式：支持常见的RowBounds(PageRowBounds)，PageHelper.startPage 方法调用，Mapper 接口参数调用。 QueryInterceptor 规范：使用 QueryInterceptor 规范，开发插件更轻松。 集成pageHelper三步完成集成： maven依赖 配置pageHelper 代码使用及测试 maven依赖此处依赖在集成了mybatis的基础上增加: &lt;!-- pagehelper --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; 配置pageHelper在application.yml里增加: # 分页配置 pagehelper: helper-dialect: mysql reasonable: &quot;true&quot; support-methods-arguments: &quot;true&quot; params: count=countSql 代码使用及测试根据官网之如何在代码中使用中的推荐，我们使用静态方法PageHelper.startPage来实现分页。 首先是SbpUserMapper接口增加方法： public interface SbpUserMapper { List&lt;SbpUser&gt; getAll(); } 对应的SbpUserMapper.xml增加sql: &lt;select id=&quot;getAll&quot; resultMap=&quot;BaseResultMap&quot;&gt; SELECT * FROM sbp_user &lt;/select&gt; 代码使用，传参当前页为1，每页条数显示5条: @Test @Rollback public void pageByPageHelper() throws Exception { PageHelper.startPage(1, 5); List&lt;SbpUser&gt; users = sbpUserMapper.getAll(); Page&lt;SbpUser&gt; page = (Page&lt;SbpUser&gt;) users; System.out.println(&quot;total count :&quot; + page.getTotal()); System.out.println(&quot;pages :&quot; + page.getPages()); System.out.println(&quot;data :&quot; + page.getResult()); } 输出: total count :21 pages :5 data :Page{count=true, pageNum=1, pageSize=5, startRow=0, endRow=5, total=21, pages=5, reasonable=true, pageSizeZero=false} 还有更多的用法请参考官方文档。 结束语pageHelper还是非常好用的，简单，集成方便，是和mybatis搭配的分页插件首选。除此之外，大家一定要会自行写分页接口，除此之外还要 知其然，知其所以然。看其源码实现与设计理念，得到的不仅仅是分页。]]></content>
      <categories>
        <category>spring-boot</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot集成Log4j2]]></title>
    <url>%2F2018%2F07%2F13%2Fspring-boot%E9%9B%86%E6%88%90Log4j2%2F</url>
    <content type="text"><![CDATA[前言在企业级项目里，日志框架种类杂乱不一，尤其是接手一些有些年头的老项目时，各种各样的日志框架依赖和使用让人看的头大。 如何解决各种各样日志框架的大一统呢？神器Slf4j。 spring-boot如何利用Slf4j集成其他日志框架？本文将以Log4j2为例。 Slf4j简单介绍去Slf4j官网瞧一瞧: The Simple Logging Facade for Java (SLF4J) serves as a simple facade or abstraction for various logging frameworks (e.g. java.util.logging, logback, log4j) allowing the end user to plug in the desired logging framework at deployment time. Slf4j:Simple Logging Facade for Java.它并不是一个类似java.util.logging, logback, log4j等的日志框架实现，而是作为一个门面服务于这些各种日志框架。如同字面含义，类似外观设计模式，提供了一组简单统一的API接口，隐藏了各种日志不一致的复杂性。 日志框架选型几种日志框架简单对比: log4j:元老级日志框架。它定义的Logger、Appender、Level等概念如今已经被广泛使用，里程碑式的日志框架 java.util.logging:简称j.u.l。java1.4版本引入，功能不如log4j，性能和可用性有限。 logback：log4j升级版。它比log4j增加了不少功能，比如：原生实现了Slf4J,支持XML、Groovy方式配置等，主要是性能比log4j提升不少。 log4j2:同样是log4j升级版。也比log4j添加不少功能，比如多线程下的异步日志等。性能也提升不少。 至于logback和log4j2性能比对，可以参考网上博文： logback log4j log4j2 性能实测 看log4j2如何秒杀一切日志组件 根据官方推荐是Slf4j+logback，此处我使用Slf4j2+log4j2来集成日志框架。 集成Log4j2三步完成Log4j2的集成: maven依赖 配置日志文件 测试日志 maven依赖pom.xml： &lt;!-- 需要排除logback --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- log4j2 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;/dependency&gt; 配置日志文件classpath下添加log4j2.xml: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;configuration status=&quot;OFF&quot;&gt; &lt;properties&gt; &lt;property name=&quot;LOG_HOME&quot;&gt;../logs/&lt;/property&gt; &lt;property name=&quot;STDOUT_FILE_NAME&quot;&gt;stdout&lt;/property&gt; &lt;/properties&gt; &lt;appenders&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;PatternLayout pattern=&quot;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot;/&gt; &lt;/Console&gt; &lt;RollingRandomAccessFile name=&quot;stdout-log&quot; fileName=&quot;${LOG_HOME}/${STDOUT_FILE_NAME}.log&quot; filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/${STDOUT_FILE_NAME}-%d{yyyy-MM-dd}-%i.log.gz&quot;&gt; &lt;PatternLayout pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot;/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot;/&gt; &lt;/RollingRandomAccessFile&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;root level=&quot;error&quot;&gt; &lt;AppenderRef ref=&quot;stdout-log&quot;/&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;logger name=&quot;com.crw&quot; level=&quot;info&quot; additivity=&quot;false&quot;&gt; &lt;AppenderRef ref=&quot;stdout-log&quot;/&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;/logger&gt; &lt;/loggers&gt; &lt;/configuration&gt; 修改应用日志文件路径，application.yml: logging.config: classpath:log4j2.xml 测试日志增加一个EchoController: @RestController @Slf4j public class EchoController { @GetMapping(&quot;/echo/hello&quot;) public String echo(String msg) { log.debug(&quot;DEBUG ----&gt; echo:{}&quot;, msg); log.info(&quot;INFO ----&gt; echo:{}&quot;, msg); log.warn(&quot;WARN ----&gt; echo:{}&quot;, msg); log.error(&quot;ERROR ----&gt; echo:{}&quot;, msg); return &quot;hello, &quot; + msg; } } 调用接口地址： curl http://127.0.0.1:8080/echo/hello?msg=world 页面输出： hello, world stdout.log文件输出： 2018-07-12 22:56:04.023 INFO [http-nio-8080-exec-3][EchoController.java:17] - INFO ----&gt; echo:world 2018-07-12 22:56:04.024 WARN [http-nio-8080-exec-3][EchoController.java:18] - WARN ----&gt; echo:world 2018-07-12 22:56:04.024 ERROR [http-nio-8080-exec-3][EchoController.java:19] - ERROR ----&gt; echo:world debug级别的日志并未输出和打印在控制台，info级别以上日志打印并输出控制台。可见日志测试成功。 结束语本文简单介绍了日志框架的种类已经统一日志API框架Slf4j,除此之外，apache下也有与Slf4j同一职能的框架commons-logging。之后可自行研究一下。]]></content>
      <categories>
        <category>spring-boot</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
        <tag>log4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-boot集成mybatis]]></title>
    <url>%2F2018%2F07%2F12%2Fspring-boot%E9%9B%86%E6%88%90mybatis%2F</url>
    <content type="text"><![CDATA[前言如果你是一个j2ee开发工程师，你一定不能不会spring，你一定不能不了解spring-boot，你一定不能不知道最火的orm框架Mybatis。 本文使用spring-boot集成mybatis，体会下spring-boot + mybatis实现效率开发数据层代码。 一分钟创建工程通过idea创建spring-boot项目，File-&gt;New-&gt;Project...:点击Next配置你的项目的基础信息，再点击Next，勾选需要的依赖:创建完毕。耗时不到半分钟。 对于非idea使用的用户，可以直接登录https://start.spring.io,用同样的方式打包你的程序即可。 mybatis自动生成代码通过mybatis-generator技术来自动生成数据库层相关代码。 大致分为三步： 创建表结构。 配置 generatorConfig.xml 依赖 mybatis-generator-maven-plugin 并运行创建 创建表结构创建一个简单的用户表： CREATE TABLE `sbp_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `nick_name` varchar(50) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;昵称&#39;, `password` varchar(50) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;密码&#39;, `mobile` varchar(15) DEFAULT NULL COMMENT &#39;手机号码&#39;, `create_at` bigint(20) NOT NULL, `update_at` bigint(20) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=100 DEFAULT CHARSET=utf8; 配置 generatorConfig.xml以下配置文件都放置在classpath下： generator.properties文件： ## jdbc jdbc.driverClassName=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://10.0.0.20:3306/spring-boot-practice?useUnicode=true&amp;amp;characterEncoding=utf-8 jdbc.username=root jdbc.password=root ## model model.targetPackage=com.crw.model ## DAO dao.targetPackage=com.crw.mapper dao.type=XMLMAPPER ## table.name=product_honor table.name=sbp_user generatorConfig.xml文件: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt; &lt;generatorConfiguration&gt; &lt;properties resource=&quot;generator.properties&quot;&gt;&lt;/properties&gt; &lt;context id=&quot;mysql&quot; defaultModelType=&quot;flat&quot; targetRuntime=&quot;MyBatis3&quot;&gt; &lt;property name=&quot;javaFileEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;commentGenerator&gt; &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot;/&gt; &lt;/commentGenerator&gt; &lt;jdbcConnection driverClass=&quot;${jdbc.driverClassName}&quot; connectionURL=&quot;${jdbc.url}&quot; userId=&quot;${jdbc.username}&quot; password=&quot;${jdbc.password}&quot;/&gt; &lt;javaTypeResolver&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;javaModelGenerator targetPackage=&quot;${model.targetPackage}&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;property name=&quot;constructorBased&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;immutable&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;src/main/resources&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;/sqlMapGenerator&gt; &lt;javaClientGenerator targetPackage=&quot;${dao.targetPackage}&quot; targetProject=&quot;src/main/java&quot; type=&quot;${dao.type}&quot;&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;&quot;/&gt; &lt;property name=&quot;exampleMethodVisibility&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;methodNameCalculator&quot; value=&quot;&quot;/&gt; &lt;property name=&quot;rootInterface&quot; value=&quot;&quot;/&gt; &lt;/javaClientGenerator&gt; &lt;table tableName=&quot;${table.name}&quot; schema=&quot;wealth&quot; enableUpdateByExample=&quot;true&quot;&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt; 附录：mybatis generator 配置详解，参考资料：http://www.jianshu.com/p/e09d2370b796 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE generatorConfiguration PUBLIC &quot;-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd&quot;&gt; &lt;!-- 配置生成器 --&gt; &lt;generatorConfiguration&gt; &lt;!-- 可以用于加载配置项或者配置文件，在整个配置文件中就可以使用${propertyKey}的方式来引用配置项 resource：配置资源加载地址，使用resource，MBG从classpath开始找，比如com/myproject/generatorConfig.properties url：配置资源加载地质，使用URL的方式，比如file:///C:/myfolder/generatorConfig.properties. 注意，两个属性只能选址一个; 另外，如果使用了mybatis-generator-maven-plugin，那么在pom.xml中定义的properties都可以直接在generatorConfig.xml中使用 &lt;properties resource=&quot;&quot; url=&quot;&quot; /&gt; --&gt; &lt;!-- 在MBG工作的时候，需要额外加载的依赖包 location属性指明加载jar/zip包的全路径 &lt;classPathEntry location=&quot;/Program Files/IBM/SQLLIB/java/db2java.zip&quot; /&gt; --&gt; &lt;!-- context:生成一组对象的环境 id:必选，上下文id，用于在生成错误时提示 defaultModelType:指定生成对象的样式 1，conditional：类似hierarchical； 2，flat：所有内容（主键，blob）等全部生成在一个对象中； 3，hierarchical：主键生成一个XXKey对象(key class)，Blob等单独生成一个对象，其他简单属性在一个对象中(record class) targetRuntime: 1，MyBatis3：默认的值，生成基于MyBatis3.x以上版本的内容，包括XXXBySample； 2，MyBatis3Simple：类似MyBatis3，只是不生成XXXBySample； introspectedColumnImpl：类全限定名，用于扩展MBG --&gt; &lt;context id=&quot;mysql&quot; defaultModelType=&quot;hierarchical&quot; targetRuntime=&quot;MyBatis3Simple&quot; &gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name=&quot;autoDelimitKeywords&quot; value=&quot;false&quot;/&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name=&quot;javaFileEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name=&quot;javaFormatter&quot; value=&quot;org.mybatis.generator.api.dom.DefaultJavaFormatter&quot;/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name=&quot;xmlFormatter&quot; value=&quot;org.mybatis.generator.api.dom.DefaultXmlFormatter&quot;/&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot;/&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot;/&gt; &lt;!-- 必须要有的，使用这个配置链接数据库 @TODO:是否可以扩展 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql:///pss&quot; userId=&quot;root&quot; password=&quot;admin&quot;&gt; &lt;!-- 这里面可以设置property属性，每一个property属性都设置到配置的Driver上 --&gt; &lt;/jdbcConnection&gt; &lt;!-- java类型处理器 用于处理DB中的类型到Java中的类型，默认使用JavaTypeResolverDefaultImpl； 注意一点，默认会先尝试使用Integer，Long，Short等来对应DECIMAL和 NUMERIC数据类型； --&gt; &lt;javaTypeResolver type=&quot;org.mybatis.generator.internal.types.JavaTypeResolverDefaultImpl&quot;&gt; &lt;!-- true：使用BigDecimal对应DECIMAL和 NUMERIC数据类型 false：默认, scale&gt;0;length&gt;18：使用BigDecimal; scale=0;length[10,18]：使用Long； scale=0;length[5,9]：使用Integer； scale=0;length&lt;5：使用Short； --&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;!-- java模型创建器，是必须要的元素 负责：1，key类（见context的defaultModelType）；2，java类；3，查询类 targetPackage：生成的类要放的包，真实的包受enableSubPackages属性控制； targetProject：目标项目，指定一个存在的目录下，生成的内容会放到指定目录中，如果目录不存在，MBG不会自动建目录 --&gt; &lt;javaModelGenerator targetPackage=&quot;com._520it.mybatis.domain&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;!-- for MyBatis3/MyBatis3Simple 自动为每一个生成的类创建一个构造方法，构造方法包含了所有的field；而不是使用setter； --&gt; &lt;property name=&quot;constructorBased&quot; value=&quot;false&quot;/&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;!-- for MyBatis3 / MyBatis3Simple 是否创建一个不可变的类，如果为true， 那么MBG会创建一个没有setter方法的类，取而代之的是类似constructorBased的类 --&gt; &lt;property name=&quot;immutable&quot; value=&quot;false&quot;/&gt; &lt;!-- 设置一个根对象， 如果设置了这个根对象，那么生成的keyClass或者recordClass会继承这个类；在Table的rootClass属性中可以覆盖该选项 注意：如果在key class或者record class中有root class相同的属性，MBG就不会重新生成这些属性了，包括： 1，属性名相同，类型相同，有相同的getter/setter方法； --&gt; &lt;property name=&quot;rootClass&quot; value=&quot;com._520it.mybatis.domain.BaseDomain&quot;/&gt; &lt;!-- 设置是否在getter方法中，对String类型字段调用trim()方法 --&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成SQL map的XML文件生成器， 注意，在Mybatis3之后，我们可以使用mapper.xml文件+Mapper接口（或者不用mapper接口）， 或者只使用Mapper接口+Annotation，所以，如果 javaClientGenerator配置中配置了需要生成XML的话，这个元素就必须配置 targetPackage/targetProject:同javaModelGenerator --&gt; &lt;sqlMapGenerator targetPackage=&quot;com._520it.mybatis.mapper&quot; targetProject=&quot;src/main/resources&quot;&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 对于mybatis来说，即生成Mapper接口，注意，如果没有配置该元素，那么默认不会生成Mapper接口 targetPackage/targetProject:同javaModelGenerator type：选择怎么生成mapper接口（在MyBatis3/MyBatis3Simple下）： 1，ANNOTATEDMAPPER：会生成使用Mapper接口+Annotation的方式创建（SQL生成在annotation中），不会生成对应的XML； 2，MIXEDMAPPER：使用混合配置，会生成Mapper接口，并适当添加合适的Annotation，但是XML会生成在XML中； 3，XMLMAPPER：会生成Mapper接口，接口完全依赖XML； 注意，如果context是MyBatis3Simple：只支持ANNOTATEDMAPPER和XMLMAPPER --&gt; &lt;javaClientGenerator targetPackage=&quot;com._520it.mybatis.mapper&quot; type=&quot;ANNOTATEDMAPPER&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;!-- 在targetPackage的基础上，根据数据库的schema再生成一层package，最终生成的类放在这个package下，默认为false --&gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;true&quot;/&gt; &lt;!-- 可以为所有生成的接口添加一个父接口，但是MBG只负责生成，不负责检查 &lt;property name=&quot;rootInterface&quot; value=&quot;&quot;/&gt; --&gt; &lt;/javaClientGenerator&gt; &lt;!-- 选择一个table来生成相关文件，可以有一个或多个table，必须要有table元素 选择的table会生成一下文件： 1，SQL map文件 2，生成一个主键类； 3，除了BLOB和主键的其他字段的类； 4，包含BLOB的类； 5，一个用户生成动态查询的条件类（selectByExample, deleteByExample），可选； 6，Mapper接口（可选） tableName（必要）：要生成对象的表名； 注意：大小写敏感问题。正常情况下，MBG会自动的去识别数据库标识符的大小写敏感度，在一般情况下，MBG会 根据设置的schema，catalog或tablename去查询数据表，按照下面的流程： 1，如果schema，catalog或tablename中有空格，那么设置的是什么格式，就精确的使用指定的大小写格式去查询； 2，否则，如果数据库的标识符使用大写的，那么MBG自动把表名变成大写再查找； 3，否则，如果数据库的标识符使用小写的，那么MBG自动把表名变成小写再查找； 4，否则，使用指定的大小写格式查询； 另外的，如果在创建表的时候，使用的&quot;&quot;把数据库对象规定大小写，就算数据库标识符是使用的大写，在这种情况下也会使用给定的大小写来创建表名； 这个时候，请设置delimitIdentifiers=&quot;true&quot;即可保留大小写格式； 可选： 1，schema：数据库的schema； 2，catalog：数据库的catalog； 3，alias：为数据表设置的别名，如果设置了alias，那么生成的所有的SELECT SQL语句中，列名会变成：alias_actualColumnName 4，domainObjectName：生成的domain类的名字，如果不设置，直接使用表名作为domain类的名字；可以设置为somepck.domainName，那么会自动把domainName类再放到somepck包里面； 5，enableInsert（默认true）：指定是否生成insert语句； 6，enableSelectByPrimaryKey（默认true）：指定是否生成按照主键查询对象的语句（就是getById或get）； 7，enableSelectByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询语句； 8，enableUpdateByPrimaryKey（默认true）：指定是否生成按照主键修改对象的语句（即update)； 9，enableDeleteByPrimaryKey（默认true）：指定是否生成按照主键删除对象的语句（即delete）； 10，enableDeleteByExample（默认true）：MyBatis3Simple为false，指定是否生成动态删除语句； 11，enableCountByExample（默认true）：MyBatis3Simple为false，指定是否生成动态查询总条数语句（用于分页的总条数查询）； 12，enableUpdateByExample（默认true）：MyBatis3Simple为false，指定是否生成动态修改语句（只修改对象中不为空的属性）； 13，modelType：参考context元素的defaultModelType，相当于覆盖； 14，delimitIdentifiers：参考tableName的解释，注意，默认的delimitIdentifiers是双引号，如果类似MYSQL这样的数据库，使用的是`（反引号，那么还需要设置context的beginningDelimiter和endingDelimiter属性） 15，delimitAllColumns：设置是否所有生成的SQL中的列名都使用标识符引起来。默认为false，delimitIdentifiers参考context的属性 注意，table里面很多参数都是对javaModelGenerator，context等元素的默认属性的一个复写； --&gt; &lt;table tableName=&quot;userinfo&quot; &gt; &lt;!-- 参考 javaModelGenerator 的 constructorBased属性--&gt; &lt;property name=&quot;constructorBased&quot; value=&quot;false&quot;/&gt; &lt;!-- 默认为false，如果设置为true，在生成的SQL中，table名字不会加上catalog或schema； --&gt; &lt;property name=&quot;ignoreQualifiersAtRuntime&quot; value=&quot;false&quot;/&gt; &lt;!-- 参考 javaModelGenerator 的 immutable 属性 --&gt; &lt;property name=&quot;immutable&quot; value=&quot;false&quot;/&gt; &lt;!-- 指定是否只生成domain类，如果设置为true，只生成domain类，如果还配置了sqlMapGenerator，那么在mapper XML文件中，只生成resultMap元素 --&gt; &lt;property name=&quot;modelOnly&quot; value=&quot;false&quot;/&gt; &lt;!-- 参考 javaModelGenerator 的 rootClass 属性 &lt;property name=&quot;rootClass&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- 参考javaClientGenerator 的 rootInterface 属性 &lt;property name=&quot;rootInterface&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- 如果设置了runtimeCatalog，那么在生成的SQL中，使用该指定的catalog，而不是table元素上的catalog &lt;property name=&quot;runtimeCatalog&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- 如果设置了runtimeSchema，那么在生成的SQL中，使用该指定的schema，而不是table元素上的schema &lt;property name=&quot;runtimeSchema&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- 如果设置了runtimeTableName，那么在生成的SQL中，使用该指定的tablename，而不是table元素上的tablename &lt;property name=&quot;runtimeTableName&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- 注意，该属性只针对MyBatis3Simple有用； 如果选择的runtime是MyBatis3Simple，那么会生成一个SelectAll方法，如果指定了selectAllOrderByClause，那么会在该SQL中添加指定的这个order条件； --&gt; &lt;property name=&quot;selectAllOrderByClause&quot; value=&quot;age desc,username asc&quot;/&gt; &lt;!-- 如果设置为true，生成的model类会直接使用column本身的名字，而不会再使用驼峰命名方法，比如BORN_DATE，生成的属性名字就是BORN_DATE,而不会是bornDate --&gt; &lt;property name=&quot;useActualColumnNames&quot; value=&quot;false&quot;/&gt; &lt;!-- generatedKey用于生成生成主键的方法， 如果设置了该元素，MBG会在生成的&lt;insert&gt;元素中生成一条正确的&lt;selectKey&gt;元素，该元素可选 column:主键的列名； sqlStatement：要生成的selectKey语句，有以下可选项： Cloudscape:相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2 :相当于selectKey的SQL为： VALUES IDENTITY_VAL_LOCAL() DB2_MF :相当于selectKey的SQL为：SELECT IDENTITY_VAL_LOCAL() FROM SYSIBM.SYSDUMMY1 Derby :相当于selectKey的SQL为：VALUES IDENTITY_VAL_LOCAL() HSQLDB :相当于selectKey的SQL为：CALL IDENTITY() Informix :相当于selectKey的SQL为：select dbinfo(&#39;sqlca.sqlerrd1&#39;) from systables where tabid=1 MySql :相当于selectKey的SQL为：SELECT LAST_INSERT_ID() SqlServer :相当于selectKey的SQL为：SELECT SCOPE_IDENTITY() SYBASE :相当于selectKey的SQL为：SELECT @@IDENTITY JDBC :相当于在生成的insert元素上添加useGeneratedKeys=&quot;true&quot;和keyProperty属性 &lt;generatedKey column=&quot;&quot; sqlStatement=&quot;&quot;/&gt; --&gt; &lt;!-- 该元素会在根据表中列名计算对象属性名之前先重命名列名，非常适合用于表中的列都有公用的前缀字符串的时候， 比如列名为：CUST_ID,CUST_NAME,CUST_EMAIL,CUST_ADDRESS等； 那么就可以设置searchString为&quot;^CUST_&quot;，并使用空白替换，那么生成的Customer对象中的属性名称就不是 custId,custName等，而是先被替换为ID,NAME,EMAIL,然后变成属性：id，name，email； 注意，MBG是使用java.util.regex.Matcher.replaceAll来替换searchString和replaceString的， 如果使用了columnOverride元素，该属性无效； &lt;columnRenamingRule searchString=&quot;&quot; replaceString=&quot;&quot;/&gt; --&gt; &lt;!-- 用来修改表中某个列的属性，MBG会使用修改后的列来生成domain的属性； column:要重新设置的列名； 注意，一个table元素中可以有多个columnOverride元素哈~ --&gt; &lt;columnOverride column=&quot;username&quot;&gt; &lt;!-- 使用property属性来指定列要生成的属性名称 --&gt; &lt;property name=&quot;property&quot; value=&quot;userName&quot;/&gt; &lt;!-- javaType用于指定生成的domain的属性类型，使用类型的全限定名 &lt;property name=&quot;javaType&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- jdbcType用于指定该列的JDBC类型 &lt;property name=&quot;jdbcType&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- typeHandler 用于指定该列使用到的TypeHandler，如果要指定，配置类型处理器的全限定名 注意，mybatis中，不会生成到mybatis-config.xml中的typeHandler 只会生成类似：where id = #{id,jdbcType=BIGINT,typeHandler=com._520it.mybatis.MyTypeHandler}的参数描述 &lt;property name=&quot;jdbcType&quot; value=&quot;&quot;/&gt; --&gt; &lt;!-- 参考table元素的delimitAllColumns配置，默认为false &lt;property name=&quot;delimitedColumnName&quot; value=&quot;&quot;/&gt; --&gt; &lt;/columnOverride&gt; &lt;!-- ignoreColumn设置一个MGB忽略的列，如果设置了改列，那么在生成的domain中，生成的SQL中，都不会有该列出现 column:指定要忽略的列的名字； delimitedColumnName：参考table元素的delimitAllColumns配置，默认为false 注意，一个table元素中可以有多个ignoreColumn元素 &lt;ignoreColumn column=&quot;deptId&quot; delimitedColumnName=&quot;&quot;/&gt; --&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt; 通过mybatis-generator-maven-plugin插件生产代码配置maven依赖： &lt;build&gt; &lt;plugins&gt; ... &lt;!-- mybatis generator--&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.4&lt;/version&gt; &lt;configuration&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.41&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 运行命令： mvn mybatis-generator:generate 即可生成相应的代码至相应的配置目录。 集成mybatis三步完成集成： 添加maven依赖 配置数据源和Mybatis 单元测试 添加maven依赖依赖spring-boot相关以及jdbc相关： &lt;dependencies&gt; &lt;!-- spring-boot相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.41&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.18&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 配置数据源和Mybatis配置spring-boot的application.yml(如果喜欢用application.properties)的改成相应的格式即可。 如下创建连接池和mybatis配置： # jdbc-DruidDataSource连接池配置 sbp.datasource: type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/spring-boot-practice?useUnicode=true&amp;amp;characterEncoding=utf8mb4&amp;amp;useSSL=false&amp;amp;autoReconnect=true username: root password: root # mybatis mybatis: mapperLocations: classpath:mapper/*.xml configuration.mapUnderscoreToCamelCase: true 创建DataSource,映射配置文件内容，扫描mapper接口： @Configuration @MapperScan(basePackages = &quot;com.crw.mapper&quot;) public class DataSourceConfig { @Bean @ConfigurationProperties(prefix = &quot;sbp.datasource&quot;) public DataSource datasource() { return new DruidDataSource(); } } 单元测试测试mybatis是否正常: @RunWith(SpringJUnit4ClassRunner.class) @SpringBootTest(classes = App.class) public class SbpUserMapperTest { @Autowired private SbpUserMapper sbpUserMapper; @Test @Rollback public void insert() throws Exception { long now = System.currentTimeMillis(); int id = sbpUserMapper.insert(new SbpUser(1L, &quot;张三&quot;, &quot;111111&quot;, &quot;11100001001&quot;, now, now)); Assert.assertEquals(id, 1L); } @Test @Rollback public void selectByPrimaryKey() throws Exception { SbpUser user = sbpUserMapper.selectByPrimaryKey(1L); Assert.assertEquals(&quot;张三&quot;, user.getNickName()); } @Test @Rollback public void update() throws Exception { long now = System.currentTimeMillis(); sbpUserMapper.updateByPrimaryKey(new SbpUser(1L, &quot;张三改&quot;, &quot;111111&quot;, &quot;11100001001&quot;, now, now)); SbpUser user = sbpUserMapper.selectByPrimaryKey(1L); Assert.assertEquals(&quot;张三改&quot;, user.getNickName()); } @Test @Rollback public void selectByExample() throws Exception { SbpUserExample example = new SbpUserExample(); SbpUserExample.Criteria criteria = example.createCriteria(); criteria.andMobileEqualTo(&quot;11100001001&quot;); List&lt;SbpUser&gt; users = sbpUserMapper.selectByExample(example); Assert.assertEquals(users.size(), 1); } } 运行之后正常CRUD，完成spring-boot与Mybatis的集成。 结束语本篇文章从创建spring-boot项目开始介绍了集成mybatis的过程。可以见到spring-boot是多么的高效： 一分钟(半分钟)创建spring-boot项目 三步完成myybatis代码自动生成 三步配置集成mybatis 前后大概10分钟时间绰绰有余了。效率是不是不差于世界上最好的PHP呢？(:D)]]></content>
      <categories>
        <category>spring-boot</category>
      </categories>
      <tags>
        <tag>spring-boot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发修行之基础篇：线程安全]]></title>
    <url>%2F2018%2F07%2F09%2Fjava%E5%B9%B6%E5%8F%91%E4%BF%AE%E8%A1%8C%E4%B9%8B%E5%9F%BA%E7%A1%80%E7%AF%87%EF%BC%9A%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[前言在互联网应用广泛的今天，软件并发已经成为目前软件开发的必备基础。java作为一门成熟的语言，其拥有着极其高效的并发机制，是目前大中型企业的常用开发语言。想要开发大规模应用，java并发已成为java程序猿们的必备基础技能。 从今天开始，开启java并发修行之路。 什么是线程安全性线程的安全性总是难以定义的。在阅读《java并发编程实战》的过程中觉得说的很好： 在线程安全性的定义中，最核心的概念就是正确性。 何为正确性？ 某个类的行为与其规范完全一致。 通常我们并不规定类的规范，于是我们通俗对正确性的理解是，单线程的类的行为是按照我们“所见”来运行的，我们确保其可信，“所见即所知”。 于是给出线程安全性的定义： 当多个线程访问某个类时，这个类始终都能表现出正确的行为。 某个对象保证多线程环境下共享、可修改的状态的正确性，那么即是线程安全。 换个角度分析： 当某个类单线程条件都不是正确的，那么其肯定不是线程安全的。 无状态对象一定线程安全 状态不共享或不可修改，即不存在线程安全问题。 如何做到线程安全线程安全要保证： 原子性。保证一组相关操作在竞态条件下保证结果可靠。 可见性。当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性。避免指令重排序。 解释一下上面三个安全特性： 原子性关键词：一组相关操作、竞态条件。 先解释竞态条件： 当某个计算的正确性取决于多个线程的交替执行时序时，就会发生竞态条件。 简单的说，其本质就是基于了一个错误的状态去判断或执行计算。 上代码举例。比如一个多线程累加并打印偶数的程序。 程序A：典型的竞态条件无处理: public class EchoEvenNumService implements Runnable { private int num; @Override public void run() { if (num % 2 == 0) { try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(num + &quot;\t&quot;); } num++; } } 运行main程序: public static void main(String[] args) { EchoEvenNumService service = new EchoEvenNumService(); for (int i = 0; i &lt; 100; i++) { new Thread(service).start(); } } 结果是可想而知的，奇数偶数都有： 0 1 1 2 4 3 2 1 1 9 9 10 9 9 9 9 10 9 18 18 18 20 21 22 20 20 19 ... 程序B有些同学会觉得，num改成线程安全的类型(AtomicInteger)就可以了，可是事实是这样吗？修改程序A: public class EchoEvenNumService implements Runnable { private AtomicInteger num = new AtomicInteger(0); @Override public void run() { if (num.get() % 2 == 0) { try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(num.get() + &quot;\t&quot;); } num.incrementAndGet(); } } 运行main方法后，你会发现还是奇数偶数都有： 0 0 2 3 4 5 5 5 5 7 9 6 6 13 14 14 14 15 18 14 17 20 20 17 21 25 25 26 28 28 29 30 ... 这么写是因为没有理解一组相关操作。在上面的程序中，实际上需要做到三个操作：1.num.get() % 2 == 0判断。2.打印偶数。3.num递增。 即时上面三个操作各做各的做到了原子性，但是整体并不是原子性，程序依旧会错误。 程序C做到整体的原子性，加锁同步。修改程序A： public class EchoEvenNumService implements Runnable { private int num; private ReentrantLock lock = new ReentrantLock(); @Override public void run() { try { lock.lock(); if (num % 2 == 0) { try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } System.out.print(num + &quot;\t&quot;); } num++; } finally { lock.unlock(); } } } 运行main方法后，程序终于保证了正确性: 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96 98 可见性保证共享变量有效。这里需要简单提一下Java的内存模型: 看图说话: 主内存(Main Memory),存储所有变量。 工作内存(Working Memory),保存了该线程使用到的变量的主存副本拷贝。 线程、工作内存、主存三者关系：线程对变量的所有操作(读写等)都必须在工作内存中进行，而不能之间读写主内存中的变量。 如此可见，如果程序没有保证可见性，会使一部分线程读取到的是工作内存中的值(并不一定准确)，导致程序不正确执行。 如何保证可见性？手段：加锁，volatile修饰。 有序性解释下“重排序”现象： 在没有同步的情况下，编译器、处理器以及运行时等都可能对操作的执行顺序进行一些意想不到的调整。 比如赋值两个变量: private int a; private int b; 线程A对a,b进行赋值,代码中的逻辑是这样的: a = 1; b = 2; 线程A在运行时，对a变量赋值发现a变量在主存中被其他线程加锁不能访问，线程A并不会等待锁释放，它会去尝试获取b变量，当b变量没有被占用时，线程A的执行过程就会变成这样: b = 2; a = 1; 这就是JVM内部优化导致的“指令重排序”。 重排序可能导致一些重要的状态值的读取顺序改变导致程序异常甚至会死循环发生OOM。 比如如下程序： public class NoVisibility { private static boolean ready; private static int number; private static class ReaderThread extends Thread { public void run() { while (!ready) Thread.yield(); System.out.println(number); } } public static void main(String[] args) { new ReaderThread().start(); number = 42; ready = true; } } 这个程序的诡异之处在于，ReaderThread可能永远看不到ready值，更诡异的是ReaderThread的输出可能是0，ReaderThread只读到了ready的值但没有读到number值。这一切“归功于”神奇的“重排序”。 解决方式:同步。 总结本文java并发修行的第一篇，重在基础。本文简单讲解了线程安全的“定义”，以及线程安全的一些基础概念。核心在于线程并发处理共享变量时的三点保证：原子性，可见性，有序性。细细体会之，后续准备从源码层面详细对这三点进行分析。]]></content>
      <categories>
        <category>java并发修行</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>java concurrency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从一份配置清单详解Nginx服务器配置]]></title>
    <url>%2F2018%2F07%2F04%2F%E4%BB%8E%E4%B8%80%E4%BB%BD%E9%85%8D%E7%BD%AE%E6%B8%85%E5%8D%95%E8%AF%A6%E8%A7%A3Nginx%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前言Nginx是现在企业上用的比较多的高性能的HTTP和反向代理服务器。入门Nginx一定少不了学习它的配置文件。本文将比较详细的介绍一下Nginx的各个配置项。 本文转载自CodeSheep的技术博文。 原文地址:https://my.oschina.net/hansonwang99/blog/1835408 概述在前面《Nginx服务器开箱体验》 一文中我们从开箱到体验，感受了一下Nginx服务器的魅力。Nginx是轻量级的高性能Web服务器，提供了诸如HTTP代理和反向代理、负载均衡、缓存等一系列重要特性，因而在实践之中使用广泛，笔者也在学习和实践之中。 在本文中，我们继续延续前文，从前文给出的一份示例配置清单开始，详解一下Nginx服务器的各种配置指令的作用和用法。 看到了下文中的包含了 “小猪佩琪色” 的配图了吗，嘿嘿，我们开始吧！ Nginx配置文件的整体结构 从图中可以看出主要包含以下几大部分内容： 1. 全局块该部分配置主要影响Nginx全局，通常包括下面几个部分： 配置运行Nginx服务器用户（组） worker process数 Nginx进程PID存放路径 错误日志的存放路径 配置文件的引入 2. events块该部分配置主要影响Nginx服务器与用户的网络连接，主要包括： 设置网络连接的序列化 是否允许同时接收多个网络连接 事件驱动模型的选择 最大连接数的配置 3. http块 定义MIMI-Type 自定义服务日志 允许sendfile方式传输文件 连接超时时间 单连接请求数上限 4. server块 配置网络监听 于名称的虚拟主机配置 基于IP的虚拟主机配置 5. location块 location配置 请求根目录配置 更改location的URI 网站默认首页配置 一份配置清单例析笔者按照文章：《Nginx服务器开箱体验》 中的实验，给出了一份简要的清单配置举例： 配置代码如下： user nobody nobody; worker_processes 3; error_log logs/error.log; pid logs/nginx.pid; events { use epoll; worker_connections 1024; } http { include mime.types; default_type application/octet-stream; log_format main &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;; access_log logs/access.log main; sendfile on; keepalive_timeout 65; server { listen 8088; server_name codesheep; access_log /codesheep/webserver/server1/log/access.log; error_page 404 /404.html; location /server1/location1 { root /codesheep/webserver; index index.server2-location1.htm; } location /server1/location2 { root /codesheep/webserver; index index.server2-location2.htm; } } server { listen 8089; server_name 192.168.31.177; access_log /codesheep/webserver/server2/log/access.log; error_page 404 /404.html; location /server2/location1 { root /codesheep/webserver; index index.server2-location1.htm; } location /srv2/loc2 { alias /codesheep/webserver/server2/location2/; index index.server2-location2.htm; } location = /404.html { root /codesheep/webserver/; index 404.html; } } } 接下来就来详细剖析以下配置文件中各个指令的含义⬇️ 配置文件指令配置运行Nginx服务器用户（组）指令格式:user user [group]; user：指定可以运行Nginx服务器的用户 group：可选项，可以运行Nginx服务器的用户组 如果user指令不配置或者配置为 user nobody nobody ，则默认所有用户都可以启动Nginx进程 worker process数配置Nginx服务器实现并发处理服务的关键，指令格式：worker_processes number | auto; number：Nginx进程最多可以产生的worker process数 auto：Nginx进程将自动检测 按照上文中的配置清单的实验，我们给worker_processes配置的数目是：3，启动Nginx服务器后，我们可以后台看一下主机上的Nginx进程情况： ps -aux | grep nginx 很明显，理解 worker_processes 这个指令的含义就很容易了 Nginx进程PID存放路径Nginx进程是作为系统守护进程在运行，需要在某文件中保存当前运行程序的主进程号，Nginx支持该保存文件路径的自定义 指令格式：pid file; file：指定存放路径和文件名称 如果不指定默认置于路径 logs/nginx.pid 错误日志的存放路径指定格式：error_log file | stderr; file：日志输出到某个文件file stderr：日志输出到标准错误输出 配置文件的引入指令格式：include file; 该指令主要用于将其他的Nginx配置或者第三方模块的配置引用到当前的主配置文件中 设置网络连接的序列化指令格式：accept_mutex on | off; 该指令默认为on状态，表示会对多个Nginx进程接收连接进行序列化，防止多个进程对连接的争抢。 说到该指令，首先得阐述一下什么是所谓的 “惊群问题”，可以参考 WIKI百科的解释。就Nginx的场景来解释的话大致的意思就是：当一个新网络连接来到时，多个worker进程会被同时唤醒，但仅仅只有一个进程可以真正获得连接并处理之。如果每次唤醒的进程数目过多的话，其实是会影响一部分性能的。 所以在这里，如果accept_mutex on，那么多个worker将是以串行方式来处理，其中有一个worker会被唤醒；反之若accept_mutex off，那么所有的worker都会被唤醒，不过只有一个worker能获取新连接，其它的worker会重新进入休眠状态 这个值的开关与否其实是要和具体场景挂钩的。 是否允许同时接收多个网络连接指令格式：multi_accept on | off; 该指令默认为off状态，意指每个worker process 一次只能接收一个新到达的网络连接。若想让每个Nginx的worker process都有能力同时接收多个网络连接，则需要开启此配置 事件驱动模型的选择指令格式：use model; model模型可选择项包括：select、poll、kqueue、epoll、rtsig等…… 最大连接数的配置指令格式：worker_connections number; number默认值为512，表示允许每一个worker process可以同时开启的最大连接数 定义MIME-Type指令格式： include mime.types; default_type mime-type; MIME-Type指的是网络资源的媒体类型，也即前端请求的资源类型 include指令将mime.types文件包含进来 cat mime.types 来查看mime.types文件内容，我们发现其就是一个types结构，里面包含了各种浏览器能够识别的MIME类型以及对应类型的文件后缀名字，如下所示： 自定义服务日志指令格式：access_log path [format]; path：自定义服务日志的路径 + 名称 format：可选项，自定义服务日志的字符串格式。其也可以使用 log_format 定义的格式 允许sendfile方式传输文件指令格式： sendfile on | off; sendfile_max_chunk size; 前者用于开启或关闭使用sendfile()传输文件，默认off 后者指令若size&gt;0，则Nginx进程的每个worker process每次调用sendfile()传输的数据了最大不能超出此值；若size=0则表示不限制。默认值为0 连接超时时间配置指令格式：keepalive_timeout timeout [header_timeout]; timeout 表示server端对连接的保持时间，默认75秒 header_timeout 为可选项，表示在应答报文头部的 Keep-Alive 域设置超时时间：“Keep-Alive : timeout = header_timeout” 单连接请求数上限指令格式：keepalive_requests number; 该指令用于限制用户通过某一个连接向Nginx服务器发起请求的次数 配置网络监听指令格式： 第一种：配置监听的IP地址：listen IP[:PORT]; 第二种：配置监听的端口：listen PORT; 基于名称和IP的虚拟主机配置指令格式：server_name name1 name2 ... name可以有多个并列名称，而且此处的name支持正则表达式书写实际举例： server_name ~^www\d+\.myserver\.com$ 此时表示该虚拟主机可以接收类似域名 www1.myserver.com 等的请求而拒绝 www.myserver.com 的域名请求，所以说用正则表达式可以实现更精准的控制。 至于基于IP的虚拟主机配置比较简单，不再太赘述： 指令格式：server_name IP地址 location配置指令格式为：location [ = | ~ | ~* | ^~ ] uri {...} 这里的uri分为标准uri和正则uri，两者的唯一区别是uri中是否包含正则表达式uri前面的方括号中的内容是可选项，解释如下： “=”：用于标准uri前，要求请求字符串与uri严格匹配，一旦匹配成功则停止 “~”：用于正则uri前，并且区分大小写 “~*”：用于正则uri前，但不区分大小写 “^~”：用于标准uri前，要求Nginx找到标识uri和请求字符串匹配度最高的location后，立即使用此location处理请求，而不再使用location块中的正则uri和请求字符串做匹配 请求根目录配置指令格式：root path; path：Nginx接收到请求以后查找资源的根目录路径当然，还可以通过alias指令来更改location接收到的URI请求路径，指令为： 设置网站的默认首页指令格式：index file ...... file可以包含多个用空格隔开的文件名，首先找到哪个页面，就使用哪个页面响应请求 结束语非常感谢CodeSheep的的分享！ 通过本文并结合官方文档，并加以加以使用，多多去体验Nginx的神奇吧。]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>转载摘抄</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo扩展机制实现(三)之扩展点特性]]></title>
    <url>%2F2018%2F06%2F25%2FDubbo%E6%89%A9%E5%B1%95%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0(%E4%B8%89)%E4%B9%8B%E6%89%A9%E5%B1%95%E7%82%B9%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[前言上一篇简单分析了Dubbo的扩展点机制的实现，以及其与java spi的区别与改进。 本篇文章准备从扩展点特性的角度分析一下源码。 扩展点自动装配 加载扩展点时，自动注入依赖的扩展点。加载扩展点时，扩展点实现类的成员如果为其它扩展点类型，ExtensionLoader 在会自动注入依赖的扩展点。ExtensionLoader 通过扫描扩展点实现类的所有 setter 方法来判定其成员。即 ExtensionLoader 会执行扩展点的拼装操作。 上一篇提到了Dubbo的ExtensionLoader提供了三种获取扩展点实现类的方式，其中的一种是根据名字获取扩展点实现: public T getExtension(String name) { if (name == null || name.length() == 0) throw new IllegalArgumentException(&quot;Extension name == null&quot;); if (&quot;true&quot;.equals(name)) { // 判断是否是获取默认实现 return getDefaultExtension(); } Holder&lt;Object&gt; holder = cachedInstances.get(name); // 从缓存中取 if (holder == null) { cachedInstances.putIfAbsent(name, new Holder&lt;Object&gt;()); holder = cachedInstances.get(name); } Object instance = holder.get(); if (instance == null) { synchronized (holder) { instance = holder.get(); if (instance == null) { instance = createExtension(name); // 创建缓存实例 holder.set(instance); } } } return (T) instance; } 还是用到了缓存，先是判断是否取默认实例，再是从缓存取和设置缓存。接下来看一下创建扩展点的方法createExtension(name): private T createExtension(String name) { Class&lt;?&gt; clazz = getExtensionClasses().get(name); // 加载当前Extension的所有实现, 并从中获取指定name的Extension if (clazz == null) { throw findException(name); } try { T instance = (T) EXTENSION_INSTANCES.get(clazz); // 从Extension实例缓存中获取实例 if (instance == null) { EXTENSION_INSTANCES.putIfAbsent(clazz, (T) clazz.newInstance()); instance = (T) EXTENSION_INSTANCES.get(clazz); } injectExtension(instance); // 注入扩展点信息 Set&lt;Class&lt;?&gt;&gt; wrapperClasses = cachedWrapperClasses; if (wrapperClasses != null &amp;&amp; !wrapperClasses.isEmpty()) { for (Class&lt;?&gt; wrapperClass : wrapperClasses) { instance = injectExtension((T) wrapperClass.getConstructor(type).newInstance(instance)); } } return instance; } catch (Throwable t) { throw new IllegalStateException(&quot;Extension instance(name: &quot; + name + &quot;, class: &quot; + type + &quot;) could not be instantiated: &quot; + t.getMessage(), t); } } 如上代码所示，createExtension(name)创建一个扩展点实例大致做了两件事： 调用getExtensionClasses()加载了扩展点类，并通过名字获取到扩展点的Class类对象 创建扩展点的Class类对象的实例，并调用injectExtension(instance) 注入扩展点信息 getExtensionClasses()暂且不说，比较重要的一个方法，在获取自适应扩展点的时候也会用到此方法。 injectExtension(instance) 注入扩展点信息，这里便展示了扩展点自动装配的特性。private T injectExtension(T instance) { try { if (objectFactory != null) { for (Method method : instance.getClass().getMethods()) { // 处理所有set方法 if (method.getName().startsWith(&quot;set&quot;) &amp;&amp; method.getParameterTypes().length == 1 &amp;&amp; Modifier.isPublic(method.getModifiers())) { Class&lt;?&gt; pt = method.getParameterTypes()[0]; try { // 获取setter对应的property名称 String property = method.getName().length() &gt; 3 ? method.getName().substring(3, 4).toLowerCase() + method.getName().substring(4) : &quot;&quot;; // 根据参数类型和属性名称，从 ExtensionFactory 里获取扩展点 Object object = objectFactory.getExtension(pt, property); if (object != null) { // 如果不为空，则 setter 方法的参数是扩展点类型，那么进行注入 method.invoke(instance, object); } } catch (Exception e) { logger.error(&quot;fail to inject via method &quot; + method.getName() + &quot; of interface &quot; + type.getName() + &quot;: &quot; + e.getMessage(), e); } } } } } catch (Exception e) { logger.error(e.getMessage(), e); } return instance; } 这里可以看到，扩展点自动注入就是根据setter方法对应的参数类型和property名称从ExtensionFactory中查询，如果有返回扩展点实例，那么就进行注入操作。 扩展点自适应Dubbo扩展点有一个非常重要的概念：Adaptive. ExtensionLoader 注入的依赖扩展点是一个 Adaptive 实例，直到扩展点方法执行时才决定调用是一个扩展点实现。 Dubbo 使用 URL 对象（包含了Key-Value）传递配置信息。 扩展点方法调用会有URL参数（或是参数有URL成员） 这样依赖的扩展点也可以从URL拿到配置信息，所有的扩展点自己定好配置的Key后，配置信息从URL上从最外层传入。URL在配置传递上即是一条总线。 来看看Dubbo获取自适应扩展点的方法:getAdaptiveExtension(): public T getAdaptiveExtension() { Object instance = cachedAdaptiveInstance.get(); // 从缓存中获取自适应实例 if (instance == null) { if (createAdaptiveInstanceError == null) { synchronized (cachedAdaptiveInstance) { instance = cachedAdaptiveInstance.get(); if (instance == null) { try { instance = createAdaptiveExtension(); // 创建自适应实例并缓存 cachedAdaptiveInstance.set(instance); } catch (Throwable t) { createAdaptiveInstanceError = t; throw new IllegalStateException(&quot;fail to create adaptive instance: &quot; + t.toString(), t); } } } } else { throw new IllegalStateException(&quot;fail to create adaptive instance: &quot; + createAdaptiveInstanceError.toString(), createAdaptiveInstanceError); } } return (T) instance; } 又是缓存，看看如何创建一个自适应扩展点:createAdaptiveExtension() private T createAdaptiveExtension() { try { return injectExtension((T) getAdaptiveExtensionClass().newInstance()); } catch (Exception e) { throw new IllegalStateException(&quot;Can not create adaptive extension &quot; + type + &quot;, cause: &quot; + e.getMessage(), e); } } injectExtension(instance)方法上面说明过了，是对扩展点自动装配。主要看getAdaptiveExtensionClass()方法： private Class&lt;?&gt; getAdaptiveExtensionClass() { getExtensionClasses(); // 加载当前Extension的所有实现 if (cachedAdaptiveClass != null) { return cachedAdaptiveClass; } return cachedAdaptiveClass = createAdaptiveExtensionClass(); // 动态创建自适应扩展类 Class 对象 } getExtensionClasses()是个很重要的方法，三种获取扩展点实现的方法都会用到这个方法，这个稍后说明一下。这里主要看createAdaptiveExtensionClass(): private Class&lt;?&gt; createAdaptiveExtensionClass() { String code = createAdaptiveExtensionClassCode(); // 自适应扩展类拼装代码 ClassLoader classLoader = findClassLoader(); com.alibaba.dubbo.common.compiler.Compiler compiler = ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.common.compiler.Compiler.class).getAdaptiveExtension(); return compiler.compile(code, classLoader); // 动态编译 } 这里也是获取了Compiler接口的自适应扩展点AdaptiveCompiler的实现，由于实现里又compiler = loader.getDefaultExtension()获取了默认的扩展点，即JavassistCompiler的实例，来实现了动态编译。通过断点看一下通过 javassist 生成的实现类长啥样，以Protocol的自适应扩展点来看(debug打印后格式化并做了注释方便看): package com.alibaba.dubbo.rpc; import com.alibaba.dubbo.common.extension.ExtensionLoader; public class Protocol$Adaptive implements com.alibaba.dubbo.rpc.Protocol { public void destroy() { throw new UnsupportedOperationException(&quot;method public abstract void com.alibaba.dubbo.rpc.Protocol.destroy() of interface com.alibaba.dubbo.rpc.Protocol is not adaptive method!&quot;); } public int getDefaultPort() { throw new UnsupportedOperationException(&quot;method public abstract int com.alibaba.dubbo.rpc.Protocol.getDefaultPort() of interface com.alibaba.dubbo.rpc.Protocol is not adaptive method!&quot;); } public com.alibaba.dubbo.rpc.Exporter export(com.alibaba.dubbo.rpc.Invoker arg0) throws com.alibaba.dubbo.rpc.RpcException { if (arg0 == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument == null&quot;); if (arg0.getUrl() == null) throw new IllegalArgumentException(&quot;com.alibaba.dubbo.rpc.Invoker argument getUrl() == null&quot;);com.alibaba.dubbo.common.URL url = arg0.getUrl(); // 从url中获取扩展点名称,如果没有就赋值为默认的值 String extName = ( url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol() ); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.Protocol) name from url(&quot; + url.toString() + &quot;) use keys([protocol])&quot;); // 通过名字获取扩展点实现 com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class).getExtension(extName); return extension.export(arg0); } public com.alibaba.dubbo.rpc.Invoker refer(java.lang.Class arg0, com.alibaba.dubbo.common.URL arg1) throws com.alibaba.dubbo.rpc.RpcException { if (arg1 == null) throw new IllegalArgumentException(&quot;url == null&quot;); com.alibaba.dubbo.common.URL url = arg1; // 从url中获取扩展点名称,如果没有就赋值为默认的值 String extName = ( url.getProtocol() == null ? &quot;dubbo&quot; : url.getProtocol() ); if(extName == null) throw new IllegalStateException(&quot;Fail to get extension(com.alibaba.dubbo.rpc.Protocol) name from url(&quot; + url.toString() + &quot;) use keys([protocol])&quot;); // 通过名字获取扩展点实现 com.alibaba.dubbo.rpc.Protocol extension = (com.alibaba.dubbo.rpc.Protocol)ExtensionLoader.getExtensionLoader(com.alibaba.dubbo.rpc.Protocol.class).getExtension(extName); return extension.refer(arg0, arg1); } } 从上可以看到，确实正如Dubbo所描述的那样，通过url传递配置信息。 扩展点自动包装在官方文档中有如下说明： 自动包装扩展点的 Wrapper 类。ExtensionLoader 在加载扩展点时，如果加载到的扩展点有拷贝构造函数，则判定为扩展点 Wrapper 类。 在源码里的体现，在加载扩展点文件 loadFile(Map&lt;String, Class&lt;?&gt;&gt; extensionClasses, String dir) 里会根据构造器函数进行判断是否是 Wrapper类： try { clazz.getConstructor(type); // 判断是否 Wrapper 类型 Set&lt;Class&lt;?&gt;&gt; wrappers = cachedWrapperClasses; if (wrappers == null) { cachedWrapperClasses = new ConcurrentHashSet&lt;Class&lt;?&gt;&gt;(); wrappers = cachedWrapperClasses; } wrappers.add(clazz); } catch (NoSuchMethodException e) { // 非 Wrapper 类型 ... } 其次在创建扩展点实例的时候也会根据是否是 Wrapper 类来创建相应的扩展点，这在createExtension(String name)中的体现: Set&lt;Class&lt;?&gt;&gt; wrapperClasses = cachedWrapperClasses; // 获取缓存的Wrapper类集合 // 如果是包装类则创建包装类扩展点实例 if (wrapperClasses != null &amp;&amp; !wrapperClasses.isEmpty()) { for (Class&lt;?&gt; wrapperClass : wrapperClasses) { instance = injectExtension((T) wrapperClass.getConstructor(type).newInstance(instance)); } } return instance; 那么，为什么要使用 Wrapper 类呢？ Wrapper 类同样实现了扩展点接口，但是 Wrapper 不是扩展点的真正实现。它的用途主要是用于从 ExtensionLoader返回扩展点时，包装在真正的扩展点实现外。即从 ExtensionLoader 中返回的实际上是 Wrapper 类的实例，Wrapper 持有了实际的扩展点实现类。 扩展点的 Wrapper 类可以有多个，也可以根据需要新增。 通过 Wrapper 类可以把所有扩展点公共逻辑移至 Wrapper 中。新加的 Wrapper 在所有的扩展点上添加了逻辑，有些类似 AOP，即 Wrapper 代理了扩展点。 从这里看 Dubbo 的aop实际上是装饰者设计模式 + 自适应特性的动态代理。 举个例子，在之前写Dubbo暴露过程的源码中对Protocol接口的调用过程进行了分析，在ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension();获取Protocol自适应扩展点的时序是ProtocolListenerWrapper-&gt;ProtocolFilterWrapper-&gt;DubboProtocol。 在真正暴露服务之前，对此进行了一些额外的扩展操作，通过这些层层包装使得各个类逻辑分明，代码维护性高。 扩展点自动激活先看官方文档： 对于集合类扩展点，比如：Filter, InvokerListener, ExportListener, TelnetHandler, StatusChecker 等，可以同时加载多个实现，此时，可以用自动激活来简化配置。 Dubbo实现自动激活的核心关键词:Activate。 @Documented @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.TYPE, ElementType.METHOD}) public @interface Activate { /** * 根据group匹配当前扩展点 * * @return 匹配的group名称 */ String[] group() default {}; /** * 当URL上的参数包含指定的keys时，激活当前的扩展点 * &lt;p&gt; * 举个栗子, 当使用 &lt;code&gt;@Activate(&quot;cache, validation&quot;)&lt;/code&gt;, 当URL参数上包含&lt;code&gt;cache&lt;/code&gt; 或 &lt;code&gt;validation&lt;/code&gt; 时，当前扩展点才会被激活 * &lt;/p&gt; * * @return URL 参数上的key值 */ String[] value() default {}; /** * 相对排序信息, 可选 * * @return 应当放在当前扩展点之前的扩展点列表 */ String[] before() default {}; /** * 相对排序信息, 可选 * * @return 应当放在当前扩展点之前的扩展点列表 */ String[] after() default {}; /** * 绝对排序信息, 可选 * * @return 绝对排序信息 */ int order() default 0; } 总之就是根据注解的 value 和 group 两个属性来决定是否激活。比如CacheFilter： @Activate(group = {Constants.CONSUMER, Constants.PROVIDER}, value = Constants.CACHE_KEY) public class CacheFilter implements Filter {} 当满足条件: 服务提供者 和 服务消费者 url上的参数包含cache 则激活 CacheFilter 那么，Dubbo如何使用 Activate 呢？没错，一定还记得那个方法:getActivateExtension。getActivateExtension有多个重写的方法，但实际最终会调用到如下： public List&lt;T&gt; getActivateExtension(URL url, String[] values, String group) { List&lt;T&gt; exts = new ArrayList&lt;T&gt;(); List&lt;String&gt; names = values == null ? new ArrayList&lt;String&gt;(0) : Arrays.asList(values); if (!names.contains(Constants.REMOVE_VALUE_PREFIX + Constants.DEFAULT_KEY)) { getExtensionClasses(); // 此处缓存了 cachedActivates for (Map.Entry&lt;String, Activate&gt; entry : cachedActivates.entrySet()) { String name = entry.getKey(); // 获取 可激活的扩展点的spi扩展名 Activate activate = entry.getValue(); if (isMatchGroup(group, activate.group())) { // 如果group匹配 T ext = getExtension(name); // 根据扩展点名称获取扩展点实例 // name不在 values 指定的列，且没排除name，且url上有activate的value，则激活 if (!names.contains(name) &amp;&amp; !names.contains(Constants.REMOVE_VALUE_PREFIX + name) &amp;&amp; isActive(activate, url)) { exts.add(ext); } } } Collections.sort(exts, ActivateComparator.COMPARATOR); // 排序 } List&lt;T&gt; usrs = new ArrayList&lt;T&gt;(); for (int i = 0; i &lt; names.size(); i++) { // 指定使用values的时候 String name = names.get(i); // 所有未被排除的扩展名 if (!name.startsWith(Constants.REMOVE_VALUE_PREFIX) &amp;&amp; !names.contains(Constants.REMOVE_VALUE_PREFIX + name)) { if (Constants.DEFAULT_KEY.equals(name)) { if (!usrs.isEmpty()) { exts.addAll(0, usrs); usrs.clear(); } } else { T ext = getExtension(name); usrs.add(ext); } } } if (!usrs.isEmpty()) { exts.addAll(usrs); } return exts; } 关于Activate的使用场景：当需要提供一组需要指定条件的扩展点并使用的时候。比如在ProtocolFilterWrapper的buildInvokerChain里构建一组Filter时，Dubbo是这么处理的: List&lt;Filter&gt; filters = ExtensionLoader.getExtensionLoader(Filter.class).getActivateExtension(invoker.getUrl(), key, group); 扩展点的一些核心方法在整个扩展点源码里，有一些核心的方法贯穿整个ExtensionLoader。 首先是之前常见得getExtensionClasses()这个方法里进行了： 缓存扩展点Class 从指定文件路径加载扩展点文件 创建扩展点 getExtensionClasses()里的核心方法是loadExtensionClasses. private Map&lt;String, Class&lt;?&gt;&gt; loadExtensionClasses() { final SPI defaultAnnotation = type.getAnnotation(SPI.class); // 先获取 @SPI 注解中的默认值 if (defaultAnnotation != null) { // 如果 @SPI 注解存在 value 默认值, 赋值给 cachedDefaultName 属性 String value = defaultAnnotation.value(); if (value != null &amp;&amp; (value = value.trim()).length() &gt; 0) { String[] names = NAME_SEPARATOR.split(value); if (names.length &gt; 1) { // 每个扩展点实现只能配置一个名字 throw new IllegalStateException(&quot;more than 1 default extension name on extension &quot; + type.getName() + &quot;: &quot; + Arrays.toString(names)); } if (names.length == 1) cachedDefaultName = names[0]; } } // 从配置路径中加载扩展实现类 Map&lt;String, Class&lt;?&gt;&gt; extensionClasses = new HashMap&lt;String, Class&lt;?&gt;&gt;(); loadFile(extensionClasses, DUBBO_INTERNAL_DIRECTORY); loadFile(extensionClasses, DUBBO_DIRECTORY); loadFile(extensionClasses, SERVICES_DIRECTORY); return extensionClasses; } 真正开始加载扩展点文件的方法:loadFile.下面，简单粗暴的贴源码： private void loadFile(Map&lt;String, Class&lt;?&gt;&gt; extensionClasses, String dir) { String fileName = dir + type.getName(); // 获取文件路径名 try { Enumeration&lt;java.net.URL&gt; urls; ClassLoader classLoader = findClassLoader(); // 获取了类加载器 if (classLoader != null) { urls = classLoader.getResources(fileName); } else { urls = ClassLoader.getSystemResources(fileName); } if (urls != null) { while (urls.hasMoreElements()) { java.net.URL url = urls.nextElement(); try { BufferedReader reader = new BufferedReader(new InputStreamReader(url.openStream(), &quot;utf-8&quot;)); // 也是以utf-8方式读取配置文件 try { String line = null; while ((line = reader.readLine()) != null) { // 解析每行数据 final int ci = line.indexOf(&#39;#&#39;); if (ci &gt;= 0) line = line.substring(0, ci); line = line.trim(); if (line.length() &gt; 0) { // 非注释内容 try { String name = null; int i = line.indexOf(&#39;=&#39;); if (i &gt; 0) { name = line.substring(0, i).trim(); // 配置中的 key line = line.substring(i + 1).trim(); // 配置中的 value } if (line.length() &gt; 0) { Class&lt;?&gt; clazz = Class.forName(line, true, classLoader); // 获取class对象 if (!type.isAssignableFrom(clazz)) { throw new IllegalStateException(&quot;Error when load extension class(interface: &quot; + type + &quot;, class line: &quot; + clazz.getName() + &quot;), class &quot; + clazz.getName() + &quot;is not subtype of interface.&quot;); } if (clazz.isAnnotationPresent(Adaptive.class)) { // 如果注解了@Adaptive if (cachedAdaptiveClass == null) { cachedAdaptiveClass = clazz; // 缓存 cachedAdaptiveClass } else if (!cachedAdaptiveClass.equals(clazz)) { // 只允许一个 Adaptive 实现 throw new IllegalStateException(&quot;More than 1 adaptive class found: &quot; + cachedAdaptiveClass.getClass().getName() + &quot;, &quot; + clazz.getClass().getName()); } } else { try { clazz.getConstructor(type); // 判断是否 Wrapper 类型 Set&lt;Class&lt;?&gt;&gt; wrappers = cachedWrapperClasses; if (wrappers == null) { cachedWrapperClasses = new ConcurrentHashSet&lt;Class&lt;?&gt;&gt;(); wrappers = cachedWrapperClasses; } wrappers.add(clazz); } catch (NoSuchMethodException e) { // 非 Wrapper 类型 clazz.getConstructor(); // 获取class对象的无参构造器 if (name == null || name.length() == 0) { name = findAnnotationName(clazz); if (name == null || name.length() == 0) { if (clazz.getSimpleName().length() &gt; type.getSimpleName().length() &amp;&amp; clazz.getSimpleName().endsWith(type.getSimpleName())) { name = clazz.getSimpleName().substring(0, clazz.getSimpleName().length() - type.getSimpleName().length()).toLowerCase(); } else { throw new IllegalStateException(&quot;No such extension name for the class &quot; + clazz.getName() + &quot; in the config &quot; + url); } } } String[] names = NAME_SEPARATOR.split(name); if (names != null &amp;&amp; names.length &gt; 0) { Activate activate = clazz.getAnnotation(Activate.class); if (activate != null) { cachedActivates.put(names[0], activate); // 缓存 cachedActivates } for (String n : names) { if (!cachedNames.containsKey(clazz)) { cachedNames.put(clazz, n); // 缓存 cachedNames ,每个class只对应一个名称 } Class&lt;?&gt; c = extensionClasses.get(n); if (c == null) { extensionClasses.put(n, clazz); // 放入到extensionClasses中,多个 name 可能对应一个Class } else if (c != clazz) { // 重复抛异常 throw new IllegalStateException(&quot;Duplicate extension &quot; + type.getName() + &quot; name &quot; + n + &quot; on &quot; + c.getName() + &quot; and &quot; + clazz.getName()); } } } } } } } catch (Throwable t) { IllegalStateException e = new IllegalStateException(&quot;Failed to load extension class(interface: &quot; + type + &quot;, class line: &quot; + line + &quot;) in &quot; + url + &quot;, cause: &quot; + t.getMessage(), t); exceptions.put(line, e); } } } // end of while read lines } finally { reader.close(); } } catch (Throwable t) { logger.error(&quot;Exception when load extension class(interface: &quot; + type + &quot;, class file: &quot; + url + &quot;) in &quot; + url, t); } } // end of while urls } } catch (Throwable t) { logger.error(&quot;Exception when load extension class(interface: &quot; + type + &quot;, description file: &quot; + fileName + &quot;).&quot;, t); } } 文件加载完成后，几个实例被缓存了: cachedAdaptiveClass(自适应扩展点) cachedWrapperClasses(扩展点包装类) cachedActivates(扩展点激活类) cachedNames(Class-&gt;name映射) 在获取扩展点的三个方法中会常使用缓存了的数据，由此可见Dubbo在这里的缓存优化。 结束语至此，扩展点机制大致介绍完毕，自己从源码中也体会了在扩展设计上原来还可以这么玩，收益匪浅了。 最后做个总结： 从代码层面上对扩展点的四个特性进行了分析 三种获取扩展点的方式相互结合，分别体现了不同的扩展点特点 代码层面合理的设计模式(装饰器模式，动态代理)对代码分层解耦]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA神器那些让人爱不释手的小技巧]]></title>
    <url>%2F2018%2F06%2F15%2FIntellij%20IDEA%E7%A5%9E%E5%99%A8%E9%82%A3%E4%BA%9B%E8%AE%A9%E4%BA%BA%E7%88%B1%E4%B8%8D%E9%87%8A%E6%89%8B%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[前言Intellij IDEA，一个开发者甚爱的IDE，用了这么多年，其中有些小Tips你真的了解吗？Intellij IDEA神器小技巧第二弹来了！ 本文转载自Sam哥哥聊技术的技术博文。 原文地址:https://blog.csdn.net/linsongbin1/article/details/80560332 概述在2018年5月6日写了一篇介绍IntellIJ IDEA的文章,Intellij IDEA神器居然还有这些小技巧,主要是列出一些平时大家可能没用过或者没怎么用，但是又非常好用的IntellIJ IDEA小技巧。由于篇幅原因，只是列出了一小部分，那么接下来的这篇文章，会继续补充一些IntellIJ IDEA的小技巧。 别轻易自定义快捷键有蛮多操作，IntellIJ IDEA并没有给我们设置默认快捷键，需要使用者自己去定义快捷键。比如说： Rebuild Project Compare With Branch 为了能在IntellIJ IDEA里进行无鼠标编程，很多程序员都会自定义快捷键，但是有三个地方你可能需要注意一下。 经常会出现快捷键与其他应用的快捷键冲突的情况； 自定义太多快捷键了，你也不太好记住； 使用其他同事的IDEA时(比如说帮忙定位问题)，你自定义的快捷键没法用。 其实在IntellIJ IDEA里的每个操作，都可以看出一个action。我们可以使用ctrl+shift+a来输入我们要使用的操作。比如说，上面提到的Rebuild Project。你可以先使用ctrl+shift+a快捷键，然后输入Rebuild Project并回车,即可执行你要的操作。 对我自己来说，除了基础的快捷键，ctrl+shift+a是我用最频繁的快捷键了。 使用ctrl+alt+h要小心ctrl+alt+h非常好用,但是有个坑,当同一个方法里,调用某个方法多次的时候,比如说下面的代码： public class TestService { public void test1() { System.out.println(&quot;aa&quot;); } public void test2() { test1(); } public void test3() { test1(); //无数业务操作后,再次电影test1()方法 test1(); } } 如果我们想知道有哪些地方调用了test1()方法，使用ctrl+alt+h无法正确列出来的。因为ctrl+alt+h只能告诉你调用的层次。 ctrl+alt+h只是会在某个隐蔽的地方，告诉你，test3() 方法调用了 test1() 方法两次。这样就很容易坑到开发者，因为大部分人可能不太注意后面的调用次数，导致改bug的时候，以为全部都改了呢？ 如果你想精确的列出到底哪些地方调用了test1() 方法，你需要使用alt+f7这个快捷键。 尤其是我们在阅读极其复杂的业务代码时，使用alt+f7就非常合适。 当然alt+f7也可以作用在变量上，列出某个类里，哪些地方使用了该变量。 ctrl+alt+h被问的最多的两个问题经常有同事和网友问我。 Sam哥，使用ctrl+alt+h怎么跳转到源代码，又如何重新回到ctrl+alt+h对应的视图里面。 调转到源代码其实很简单，当你使用ctrl+alt+h后，使用向下或者向上箭头，选择某个调用，然后按下f4即可跳转到源代码。 如何回到ctrl+alt+h视图这个真心被问了好几百遍，其实很简单，当你使用f4跳转到源代码后，直接使用 alt+8 就可以跳回去了。就又可以继续看下一个调用的地方了。 快速找到Controller方法如果你的项目里有非常多的 controller，里面有非常多的 http 或者 resful 方法。如何快速找到这些方法呢？这个时候，ctrl+alt+shift+n 就可以派上用场了。 比如说，你依稀记得入账单相关的接口，都有个bill的url路径，那么使用 ctrl+alt+shift+n 后，直接输入/bill即可。 当你在成千上万的Controller里寻找方法时，这一招就可以大大提高效率。 了解项目关键业务流程方法的利器-bookmark在一些创业公司里，很多核心的模块都是放置在同一个项目里的。比如说，订单相关的接口，支付相关的接口，商品相关的接口。这个时候，你可以将这些关键业务方法，使用 bookmark 统一放置到某个地方，方便你阅读。 那么如何使用快捷键来达到上面的效果呢？ public class TestService { public void test1() { System.out.println(&quot;aa&quot;); } public void test2() { test1(); } public void test3() { test1(); test1(); } } 比如像上面的方法，我想将test1()方法放置到bookmark里，可以通过如下操作来完成： 使用 ctrl+f12 ,列出该类的所有方法，然后输入 test1，将光标定位在 test1 上； 按下 f11 ,将 test1() 加入到 bookmark； 按下 shift+f11，将 bookmark 列表弹出来; 按下 ctrl+enter 修改 bookmark 名字。 只留下一个tab这个是我目前正在用的，就是整个工程里面，只有一个代码 tab。也即是说，无论你打开多少个文件，都是在同一个tab里面显示。如果这样设置了，有些网友可能会问,我想看看我最近操作哪些类了，怎么看？ 可以直接使用 ctrl+e 来显示最近操作的文件。 我是比较推荐只是保留一个代码tab的，非常简洁。如果每打开一个文件，就是一个新的tab，很快你就会乱掉，而且还得关闭部分tab。 可以通过下面的方式来设置成用一个tab显示代码。按下 ctrl+shif+a ,然后输入Editor Tabs，然后回车进入编辑页面。 然后在 Placement 那里,选择 None 如何阅读又长又臭的代码由于历史原因，项目里总会存在那种无法理解的，又长又臭的业务代码。阅读这种代码，简直就是一种煎熬。但是 在IntellIJ IDEA 里，只要使用 5 个小技巧，便可大大提高阅读质量和速度。 创建任意代码折叠块 像上面的for循环，我想直接将其折叠起来，因为代码太长的时候，使用折叠块，可以帮助你快速理清代码的主脉络。 可以将光标定位在for循环的左大括号里，然后使用 ctrl+shift+. 即可。 如果你想让这个折叠快消失，直接使用ctrl 加上一个+即可。 大括号匹配这个也非常有用，因为代码太长，某个for循环，可能已经撑满整个屏幕了。这个时候，找到某个大括号对应的另外一边就很费劲。你可以将光标定位在某个大括号一边，然后使用 ctrl+] 或者 ctrl+[ 来回定位即可。 ctrl+shift+f7结合f3ctrl+shift+f7 可以高亮某个变量，而且随着鼠标的移动，这个高亮是不会消失的(这个很重要)。然后使用f3找到下一个使用该变量的地方。 在这个代码块里，你想看看 TestTemp 类的定义，那么将光标定位在 TestTemp 上，然后直接使用 ctrl+shift+i，就会弹出如下的窗口。 按下 esc，可以关闭这个窗口。 使用alt+f7这个我在上面已经介绍过了。可以列出变量在哪些地方被使用了。 结合这5个技巧，相信可以大大提高长段代码的阅读效率。 跳到父类接口我们经常会定义一 个service 接口，比如说 UserService,然后使用一个 UserServiceImpl 类去实现 UserService 里面的接口。 public interface UserService { void test1(); } public class UserServiceImpl implements UserService { @Override public void test1() { } } 那么在UserServiceImpl里的 test1() 方法上，如何跳转到 UserService 的 test1() ,直接使用 ctrl+u 即可。 后悔药如果修改了部分代码，突然觉得不合适，使用 ctrl+z 回滚掉后。突然又觉得刚才的修改是可以的。那你可以使用 ctr+shift+z 再次恢复你刚才修改的内容。 切换皮肤最快的方式可以直接使用 ctrl,然后加上一个 ` ,就可以立刻弹出如下界面： 选择 Color Scheme，然后回车，就可以弹出修改皮肤的窗口。 结束语同样来自于 Sam哥哥聊技术 的分享，这里非常感谢！ 此篇文章同样是收藏级别的文章，多学多用，才能使coding效率大大的提高。]]></content>
      <categories>
        <category>Intellij IDEA</category>
      </categories>
      <tags>
        <tag>转载摘抄</tag>
        <tag>Intellij IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo扩展机制实现(二)之浅析ExtensionLoader]]></title>
    <url>%2F2018%2F06%2F11%2FDubbo%E6%89%A9%E5%B1%95%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0(%E4%BA%8C)%E4%B9%8B%E6%B5%85%E6%9E%90ExtensionLoader%2F</url>
    <content type="text"><![CDATA[前言之前的文章有介绍jdk SPI一些基本的使用和源码分析，既然dubbo也想使用SPI机制，为什么不直接使用jdk的SPI呢？ 上篇文章开头也提到了： Dubbo 的扩展点加载从 JDK 标准的 SPI (Service Provider Interface) 扩展点发现机制加强而来。 看看官方文档上Dubbo加强了哪些地方： 本文简单分析下Dubbo实现扩展点机制的ExtensionLoader类,分析其对比java的spi是怎么改进的。 Dubbo扩展点约定 在扩展类的 jar 包内 ，放置扩展点配置文件 META-INF/dubbo/接口全限定名，内容为：配置名=扩展实现类全限定名，多个实现类用换行符分隔。 注意：这里的配置文件是放在你自己的 jar 包内，不是 dubbo 本身的 jar 包内，Dubbo 会全 ClassPath 扫描所有 jar 包内同名的这个文件，然后进行合并 ↩ 一个自定义扩展点小例子第一步：新建一个jar包，我这里是在原先的dubbo源码包里的dubbo-rpc模块新增了一个实现 [dubbo-rpc-myrpc]： &lt;parent&gt; &lt;artifactId&gt;dubbo-rpc&lt;/artifactId&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/parent&gt; &lt;artifactId&gt;dubbo-rpc-myrpc&lt;/artifactId&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;${project.artifactId}&lt;/name&gt; &lt;description&gt;The my rpc module of dubbo project&lt;/description&gt; &lt;properties&gt; &lt;skip_maven_deploy&gt;false&lt;/skip_maven_deploy&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo-rpc-api&lt;/artifactId&gt; &lt;version&gt;${project.parent.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 第二步：以实现Protocol扩展为例，新建自定义实现: /** * MyRpcProtocol */ public class MyRpcProtocol extends AbstractProtocol implements Protocol { public static final int DEFAULT_PORT = 0; @Override public int getDefaultPort() { return DEFAULT_PORT; } @Override public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException { System.out.println(&quot;my rpc export...&quot;); return null; } @Override public &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException { System.out.println(&quot;my rpc refer...&quot;); return null; } } 第三步：在jar包里定义扩展点：META-INF/dubbo/internal/com.alibaba.dubbo.rpc.Protocol myrpc=com.alibaba.dubbo.rpc.protocol.myrpc.MyRpcProtocol 大工完成，是不是so easy… 第四步：测试。接下来测试一下，写一个提供者使用我们的自定义协议。pom.xml里引用我们的自定义包 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo-rpc-myrpc&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; 提供者xml里注册我们的协议: &lt;dubbo:protocol name=&quot;myrpc&quot; port=&quot;66666&quot;/&gt; 然后启动服务: public class Provider { public static void main(String[] args) throws Exception { //Prevent to get IPV6 address,this way only work in debug mode //But you can pass use -Djava.net.preferIPv4Stack=true,then it work well whether in debug mode or not System.setProperty(&quot;java.net.preferIPv4Stack&quot;, &quot;true&quot;); ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[]{&quot;META-INF/spring/dubbo-demo-provider.xml&quot;}); context.start(); System.in.read(); // press any key to exit } } 可以看到控制台打印了: ... [11/06/18 11:12:16:016 CST] main INFO config.AbstractConfig: [DUBBO] Export dubbo service com.alibaba.dubbo.demo.DemoService to local registry, dubbo version: 2.0.0, current host: 172.16.192.43 [11/06/18 11:12:16:016 CST] main INFO config.AbstractConfig: [DUBBO] Export dubbo service com.alibaba.dubbo.demo.DemoService to url myrpc://172.16.192.43:66666/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider2&amp;bind.ip=172.16.192.43&amp;bind.port=66666&amp;dubbo=2.0.0&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=25280&amp;qos.port=22222&amp;side=provider&amp;timestamp=1528686736167, dubbo version: 2.0.0, current host: 172.16.192.43 [11/06/18 11:12:16:016 CST] main INFO config.AbstractConfig: [DUBBO] Register dubbo service com.alibaba.dubbo.demo.DemoService url myrpc://172.16.192.43:66666/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider2&amp;bind.ip=172.16.192.43&amp;bind.port=66666&amp;dubbo=2.0.0&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=25280&amp;qos.port=22222&amp;side=provider&amp;timestamp=1528686736167 to registry registry://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-provider2&amp;dubbo=2.0.0&amp;pid=25280&amp;qos.port=22222&amp;registry=zookeeper&amp;timestamp=1528686735588, dubbo version: 2.0.0, current host: 172.16.192.43 my rpc export... Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: exporter == null at com.alibaba.dubbo.rpc.listener.ListenerExporterWrapper.&lt;init&gt;(ListenerExporterWrapper.java:40) at com.alibaba.dubbo.rpc.protocol.ProtocolListenerWrapper.export(ProtocolListenerWrapper.java:59) ... 至此说明了我们的自定义扩展点可以使用。这样以后如果需要实现自定义的一些其他扩展点，使用起来也是非常easy.这里可以体现出Dubbo的设计理念: API 与 SPI 分离 微核插件式，平等对待第三方 ExtensionLoader中的缓存Dubbo官方文档也说了，扩展点的实例化并非一次性全部加载的。所以它可能是懒加载的，用到哪个实例化哪个扩展点，其次官方文档也说了Dubbo的扩展点性能提升不少，说到性能提升下意识就是想到万能的缓存。来看看 Dubbo的扩展点加载器 ExtensionLoader是怎么实现的提高性能的。 ExtensionLoader各式各样的缓存： public class ExtensionLoader&lt;T&gt; { private static final ConcurrentMap&lt;Class&lt;?&gt;, ExtensionLoader&lt;?&gt;&gt; EXTENSION_LOADERS = new ConcurrentHashMap&lt;Class&lt;?&gt;, ExtensionLoader&lt;?&gt;&gt;(); private static final ConcurrentMap&lt;Class&lt;?&gt;, Object&gt; EXTENSION_INSTANCES = new ConcurrentHashMap&lt;Class&lt;?&gt;, Object&gt;(); private final ConcurrentMap&lt;Class&lt;?&gt;, String&gt; cachedNames = new ConcurrentHashMap&lt;Class&lt;?&gt;, String&gt;(); private final Holder&lt;Map&lt;String, Class&lt;?&gt;&gt;&gt; cachedClasses = new Holder&lt;Map&lt;String, Class&lt;?&gt;&gt;&gt;(); private final Map&lt;String, Activate&gt; cachedActivates = new ConcurrentHashMap&lt;String, Activate&gt;(); private final ConcurrentMap&lt;String, Holder&lt;Object&gt;&gt; cachedInstances = new ConcurrentHashMap&lt;String, Holder&lt;Object&gt;&gt;(); private final Holder&lt;Object&gt; cachedAdaptiveInstance = new Holder&lt;Object&gt;(); private volatile Class&lt;?&gt; cachedAdaptiveClass = null; private String cachedDefaultName; private volatile Throwable createAdaptiveInstanceError; private Set&lt;Class&lt;?&gt;&gt; cachedWrapperClasses; private Map&lt;String, IllegalStateException&gt; exceptions = new ConcurrentHashMap&lt;String, IllegalStateException&gt;(); ... } ExtensionLoader并没有提供public的构造器，获取一个ExtensionLoader实例是通过私有静态方法 getExtensionLoader(Class type) 法获取。 private ExtensionLoader(Class&lt;?&gt; type) { this.type = type; /** * 这里会存在递归调用,ExtensionFactory的objectFactory为null,其他则为AdaptiveExtensionFactory * AdaptiveExtensionFactory的factories中有SpiExtensionFactory,SpringExtensionFactory * getAdaptiveExtension()来获取一个拓展装饰类对象 * objectFactory是一个 ExtensionFactory 对象，扩展点工厂类，暂且不分析 */ objectFactory = (type == ExtensionFactory.class ? null : ExtensionLoader.getExtensionLoader(ExtensionFactory.class).getAdaptiveExtension()); } @SuppressWarnings(&quot;unchecked&quot;) public static &lt;T&gt; ExtensionLoader&lt;T&gt; getExtensionLoader(Class&lt;T&gt; type) { if (type == null) //拓展点类型非空判断 throw new IllegalArgumentException(&quot;Extension type == null&quot;); if (!type.isInterface()) { // 拓展点类型只能是接口 throw new IllegalArgumentException(&quot;Extension type(&quot; + type + &quot;) is not interface!&quot;); } if (!withExtensionAnnotation(type)) { // 必须使用@spi注解,否则抛异常 throw new IllegalArgumentException(&quot;Extension type(&quot; + type + &quot;) is not extension, because WITHOUT @&quot; + SPI.class.getSimpleName() + &quot; Annotation!&quot;); } // 使用了缓存，从缓存EXTENSION_LOADERS中获取,如果不存在则创建后加入缓存，每个扩展点有且仅有一个ExtensionLoader实例与之对应。 ExtensionLoader&lt;T&gt; loader = (ExtensionLoader&lt;T&gt;) EXTENSION_LOADERS.get(type); if (loader == null) { EXTENSION_LOADERS.putIfAbsent(type, new ExtensionLoader&lt;T&gt;(type)); loader = (ExtensionLoader&lt;T&gt;) EXTENSION_LOADERS.get(type); } return loader; } Dubbo处理缓存的一些值得学习的小细节：对线程安全方面的细节做得很好。 比如缓存都使用 ConcurrentMap 而不使用 HashMap. volatile关键字的使用。 private volatile Class&lt;?&gt; cachedAdaptiveClass = null; public class Holder&lt;T&gt; { private volatile T value; public void set(T value) { this.value = value; public T get() { return value; } } Dubbo如何改进获取spi的问题问题一：JDK 标准的 SPI 会一次性实例化扩展点所有实现，如果有扩展实现初始化很耗时，但如果没用上也加载，会很浪费资源。 答：Dubbo的ExtensionLoader提供了三种获取扩展点实现类的方式： public T getExtension(String name)根据名称获取当前扩展的指定实现 public T getAdaptiveExtension()获取当前扩展点的自适应实现 public List getActivateExtension(URL url, String[] values, String group)获取可激活的扩展点集合 这三个地方准备在下一篇扩展点自适应自动激活的分析时一并讲解一下。这里可以看到Dubbo可以直接根据key就能获取到spi对象，而java的spi只能通过遍历然后根据if判断才能获取制定的spi对象。时间复杂度O(1) 比 O(n)快不少。而且用到了就加到缓存里，不用就不需要实例化，节约资源。 问题二：如果扩展点加载失败，连扩展点的名称都拿不到了。会把真正失败的原因吃掉 答: Dubbo并不会这样，当拿不到扩展点的名字时，Dubbo会直接抛出异常： public T getExtension(String name) { if (name == null || name.length() == 0) throw new IllegalArgumentException(&quot;Extension name == null&quot;); ... } 其次，Dubbo非常好的一点，增加了默认值的设置。比如： @SPI(&quot;dubbo&quot;) public interface Protocol {} 这样就默认提供了dubbo=xxx.xxx.XxxProtocol的s实现。如果使用默认的扩展点，可以这么做： Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getDefaultExtension(); 这里的protocol对象即是com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol 问题三：增加了对扩展点 IoC 和 AOP 的支持，一个扩展点可以直接 setter 注入其它扩展点 答：这个之后分析。 总结 实现一个Dubbo自定义扩展点只需要三步。 Dubbo中的缓存设计在线程安全方面非常值得学习。 Dubbo是如何加强java的spi的，java的spi上哪些的不足被Dubbo巧妙实现了。 后记本文只是简单介绍了一下扩展点加载器ExtensionLoader。之后还有更多的源码分析它。我会从Dubbo官方文档上写的四个特性分析它并借鉴其中的一些理念。 下一篇就说说扩展点的四个特性： 扩展点自动包装，扩展点自动装配，扩展点自适应，扩展点自动激活。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo扩展机制实现(一)之java SPI]]></title>
    <url>%2F2018%2F06%2F04%2FDubbo%E6%89%A9%E5%B1%95%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0(%E4%B8%80)%E4%B9%8Bjava%20SPI%2F</url>
    <content type="text"><![CDATA[前言在之前的文章Dubbo暴露服务过程中提出了问题：@SPI这些东西究竟是什么?在Dubbo开发手册之扩展点加载中有这么解释过： Dubbo 的扩展点加载从 JDK 标准的 SPI (Service Provider Interface) 扩展点发现机制加强而来。 所以在分析Dubbo扩展机制前，先看看jdk的SPI。 什么是SPISPI 全称为 (Service Provider Interface) ,是JDK内置的一种服务提供发现机制。在面向对象设计里，我们不会针对实现编程，模块间面向接口编程来防止强耦合。java spi机制实现了一种放在程序以外的方式去动态装配模块，这就是java的服务发现。类似于ioc的思想，将模块装配放在程序外，比如xml等方式。 Dubbo框架就是借鉴了这种机制，在jdk的基础上进行了改进。 java SPI机制约定java的spi是通过ServiceLoader来加载，根据官方文档来看一下SPI机制的约定： Service实现类必须有一个无参构造器 在META-INF/services/目录中提供一个文件名称为Service接口全限定名的文件，文件内容为Service接口实现类全限定名，编码格式为UTF-8 使用java.util.ServiceLoader来动态加载Service接口的实现类。 SPI示例代码地址传送门目录结构如下:接口定义： public interface HelloWorld { void sayHello(); } 两个实现： public class HelloWorldENimpl implements HelloWorld { @Override public void sayHello() { System.out.println(&quot;hello,world!&quot;); } } public class HelloWorldCNimpl implements HelloWorld { @Override public void sayHello() { System.out.println(&quot;你好，世界！&quot;); } } 配置服务发现，在META-INF/service目录下创建文件：com.crw.demo.spi.HelloWorld，内容为接口实现类全名： com.crw.demo.spi.impl.HelloWorldENimpl com.crw.demo.spi.impl.HelloWorldCNimpl 编写调用端： public class Run { public static void main(String[] args) { ServiceLoader&lt;HelloWorld&gt; loads = ServiceLoader.load(HelloWorld.class); for (HelloWorld load : loads) { load.sayHello(); } } } 运行结果如下： ServiceLoader源码分析从 ServiceLoader.load(Class\&lt;S> service) 方法点进去看一下 public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) { ClassLoader cl = Thread.currentThread().getContextClassLoader(); // 获取类加载器 return ServiceLoader.load(service, cl); } private ServiceLoader(Class&lt;S&gt; svc, ClassLoader cl) { service = Objects.requireNonNull(svc, &quot;Service interface cannot be null&quot;); loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload(); // 开始加载 } public void reload() { providers.clear(); // 清空提供者缓存 lookupIterator = new LazyIterator(service, loader); // 创建一个懒加载的提供者发现器 } 可以看到，实际上是交给了一个私有静态内部类处理new LazyIterator(service, loader); 通过名字就像是懒加载，所以我们看看什么时候类加载器会加载SPI实现服务。 答案是遍历的时候。 ServiceLoader实现了iterator接口: public Iterator&lt;S&gt; iterator() { return new Iterator&lt;S&gt;() { Iterator&lt;Map.Entry&lt;String,S&gt;&gt; knownProviders = providers.entrySet().iterator(); public boolean hasNext() { if (knownProviders.hasNext()) return true; return lookupIterator.hasNext(); // 实际上是调用LazyIterator.hasNext()方法。 } public S next() { if (knownProviders.hasNext()) return knownProviders.next().getValue(); return lookupIterator.next(); // 实际上是调用LazyIterator.next()方法。 } public void remove() { throw new UnsupportedOperationException(); } }; } 在客户端遍历的时候，首先调用了hasNext()方法，hasNext调用了LazyIterator.hasNext(),其实际上又调用了内部方法 hasNextService() : private boolean hasNextService() { if (nextName != null) { // 如果有服务提供者名称，直接返回 return true; } if (configs == null) { try { String fullName = PREFIX + service.getName(); // META-INF/services/xxx.xxx.xxx.XxxImpl // 获取配置文件加载路径 if (loader == null) configs = ClassLoader.getSystemResources(fullName); else configs = loader.getResources(fullName); } catch (IOException x) { fail(service, &quot;Error locating configuration files&quot;, x); } } while ((pending == null) || !pending.hasNext()) { if (!configs.hasMoreElements()) { return false; } pending = parse(service, configs.nextElement()); //解析配置路径，用utf-8格式读取配置 } nextName = pending.next(); // 服务提供者名称赋值 return true; } 看一眼解析完的结构，在遍历的时候会读取配置，把服务提供者名称一次性获取： 接着，在客户端遍历的时候调用了next()方法，LazyIterator.next()方法里做了如下事情 private S nextService() { if (!hasNextService()) // 如果配置里没有服务，则会抛异常 throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try { c = Class.forName(cn, false, loader); // 反射创建了配置文件里的实现类 } catch (ClassNotFoundException x) { fail(service, &quot;Provider &quot; + cn + &quot; not found&quot;); } if (!service.isAssignableFrom(c)) { fail(service, &quot;Provider &quot; + cn + &quot; not a subtype&quot;); } try { S p = service.cast(c.newInstance()); // 创建了一个实现类的实例 providers.put(cn, p); // 放入提供者缓存中 return p; } catch (Throwable x) { fail(service, &quot;Provider &quot; + cn + &quot; could not be instantiated&quot;, x); } throw new Error(); // This cannot happen } 返回了一个服务提供者实例，就这样完成了一次SPI调用。 总结这篇文章主要介绍了jdk SPI机制： java如何编写一个SPI服务的。 ServiceLoader源码如何实现SPI服务。 本篇主要是为了Dubbo实现spi而做了铺垫。在看ServiceLoader的源码时，主要还是利用了java的类加载器 ClassLoader ，这些之后会单独写一写。鉴于鄙人才疏学浅，以上文章如有不对的地方希望大家予以指出，共同进步。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL查询性能优化]]></title>
    <url>%2F2018%2F05%2F28%2FMySQL%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[目的 查询优化的基本原则 了解MySQL执行过程 浅析MySQL优化器 MySQL语句优化小tips 理解MySQL如何查询，明白高效和低效的原因 优化数据访问分两步分析是否是低效的查询： 是否向数据库请求了不需要的数据？比如查询了过多的行，查询过多的列(select * …)，以及重复的查询。 Mysql是否在扫描额外的记录？三个指标衡量查询开销： 响应时间 扫描行数 返回的行数 重构查询的方式 一个复杂查询or多个简单查询? 切分查询(比如在删改操作时，将全量语句改成批量执行) 分解关联查询(把一条复杂的关联查询分解为多条简单查询，可以让缓存效率更高，减少锁竞争) 一张图看Mysql执行过程 客户端将查询发送到服务器； 服务器检查查询缓存，如果找到了，就从缓存中返回结果，否则进行下一步。 服务器解析，预处理和优化查询，生成执行计划。 执行引擎调用存储引擎API执行查询。 服务器将结果发送回客户端。 1.客户端服务端通信协议通信协议工作机制：“抛球”游戏。任意时刻，要么是客户端发送数据，要么是服务端发送数据。一旦一端发送数据(抛球),另一端只能完整的接受消息才能响应。 所以，我们需要限制发送信息的大小。客户端：查询语句尽量少，否则可能会抛错误异常(“MySQL server has gone away”)。 # 查看你的服务器所允许传送的最大数据 SHOW VARIABLES LIKE &#39;max_allowed_packet&#39; 服务端：当服务端开始响应客户端请求时，客户端必须完整接受整个返回结果。所以要限制查询条数，必要时查询语句用“LIMIT”限制。 通过 “SHOW [FULL] PROCESSLIST” 命令查询mysql连接时线程状态:MySQL5.7通用线程状态 2.查询缓存如果开启了缓存，MySql会检查查询缓存，进行大小写敏感的哈希查找。如果命中，判断权限没问题后会跳过所有其他阶段直接返回。 # 查看是否开启查询 SHOW VARIABLES LIKE &#39;query_cache_type&#39;; 3.解析器解析器通过关键字解析SQL，然后生成一颗对应的“解析树”，然后它使用MySQL语法规则验证和解析语句，比如关键字是否错误，顺序等。 4.预处理进一步检查解析树的合法性。比如检查数据表列是否存在，名字和别名等。最后，预处理器检查权限。 5.查询优化器(重点)优化器负责将预处理合格的语法树转化为执行计划。 MySQL使用基于成本的优化器，它将尝试预测一个查询使用某种执行计划的成本，并选择成本最小的一个。 # 查询当前会话的当前查询成本 SHOW VARIABLES LIKE &#39;Last_query_cost&#39;; Q: 那么，MySQL是如何预测成本的呢？A: MySQL根据一系列统计信息计算得来:每个表或页面个数，索引基数，索引和数据行的长度，索引分布情况等。评估成本不考虑缓存，假设读取任何数据需要一次磁盘I/O。 可惜MySQL优化器并不是万能的，有诸多因素会导致MySQL选择错误的执行计划，比如统计信息可能不准确，成本估算和实际成有差距等。 更多的了解查询优化器是怎么处理查询语句的，查看MySQL查询优化器 6.查询执行引擎MySQL根据解析优化阶段生成的执行计划给出的指令逐步执行。 7.返回结果给客户端 查询结果返回给客户端，即时查询不需要返回结果集给客户端，也亏返回一些这个查询信息，比如影响的行数。 如果查询可以被缓存，这一阶段也会存放结果入缓存。 MySQL结果集返回是一个增量、逐步返回的过程。一旦处理开始产生第一条结果时，MySQL就开始逐步返回结果集了。这样的好处是，服务器无须存储太多结果而消耗太多内存，二是客户端可以第一时间获得返回结果。 总结 当发现MySQL查询效率不高时，考虑两个因素：1.是否请求了不需要的数据？2.是否查询了额外的记录？ 对待效率不高的语句，考虑语句适当拆分成多个简单的语句。 MySQL执行过程，看图]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL查询优化器]]></title>
    <url>%2F2018%2F05%2F22%2FMySQL%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E5%99%A8%2F</url>
    <content type="text"><![CDATA[目的 了解查询优化器的优化策略 了解和查看查询优化器优化后的执行计划 了解优化器是怎么优化的 了解优化器在关联语句中的处理 了解一些查询优化器的提示 MySQL查询优化器的优化策略简单分为 静态优化 和 动态优化。 一、静态优化(“编译时优化”): 可以直接对解析树进行分析并完成优化。 优化器通过一些简单的代叔变换将WHERE条件转化为另一种等价形式。 静态优化不依赖特别的数值，比如WHERE条件里带一些常数等。 静态优化在第一次完成后一直有效，即使使用不同的参数值执行查询也不会发生变化。 二、动态优化(“运行时优化”): 动态优化和查询上下文有关或者其他因素有关，比如WHERE条件中的取值，索引中条目对应的数据行数等。 每次查询都需要重新评估。 MySQL能够处理的优化类型先说一点，MySQL优化器使用了很多复杂的优化技巧把查询转化为执行计划。“不要试着比优化器更聪明”，通常都应让MySQL按照自己优化的方式执行语句，绝大多数情况优化器都是更优的。你也可以通过EXPLAIN EXTENDED SELECT … … ; SHOW WARNINGS; 查看最终优化后的执行sql。如何使用“EXPLAIN EXTENDED”可参考官方文档上的Extended EXPLAIN Output Format 重新定义关联表顺序。数据表的关联并不总是按照查询中指定顺序进行的。 将外链接转化为内连接。优化器可以根据某些因素(比如WHERE条件，库表结构等)，使得一个外连接等价于一个内连接。 等价变换规则。MySQL通过等价变换来简化并规范表达式。MySQL可以合并和减少比较。比如 (5=5 AND a&gt;5)会被改写为a&gt;5。(a &lt; b AND b=c) AND a=5会被改写为(b&gt;5 AND b=c AND a=5) 优化 COUNT()、MIN()、MAX()。查找某列最大/最小值，该列又有索引，查找最大值，则会直接找最后一行；最小值，则直接找第一行。因为索引已经排好序了。可以从EXPLAIN中看到：“Select tables optimized away”。这说明已经从执行计划中移除该表并用常数取代。 预估并转化为常数表达式如果一个表达式可以被简化为一个常量，那么这个表达式就会被转换。 在WHERE 、USING、ON这些连接条件强制值相等的条件中，常量具有传递性`EXPLAIN SELECT table_a.id, table_b.idFROM table_bINNER JOIN table_a ON table_a.tb_id = table_b.idWHERE table_b.id = 1 ![](http://ww1.sinaimg.cn/large/87faef88ly1fqnue0bqrbj20r102ut8o.jpg) 6. &lt;font face=&quot;华文新魏&quot; size=&quot;3&quot;&gt;覆盖索引&lt;/font&gt; 当索引包含查询需要的列时，MySql就可以使用索引来避免读取行数据。 7. &lt;font face=&quot;华文新魏&quot; size=&quot;3&quot;&gt;子查询优化&lt;/font&gt; MySQL可以将某些类型的子查询转换成相等的效率更高的形式。 8. &lt;font face=&quot;华文新魏&quot; size=&quot;3&quot;&gt;提前终止查询&lt;/font&gt; - MySQL在发现已经满足查询需求时，会立刻终止查询。 - MySQL检测一个不成立的条件也会立刻返回空结果。 EXPLAIN SELECT * FROM table_a WHERE id = - 1; ![](http://ww1.sinaimg.cn/large/87faef88ly1fqongff9j4j20sx01rjra.jpg) 9. &lt;font face=&quot;华文新魏&quot; size=&quot;3&quot;&gt;等值传播&lt;/font&gt; 如果两个列的值通过等式关联，那么MySQL能把其中一个列的WHERE条件传递到另一个列。 比如： SELECT table_a.id, table_b.id FROM table_b INNER JOIN table_a ON table_a.tb_id = table_b.id WHERE table_b.id &lt; 100; MySQL会判断把WHERE后面的关联作用于table_a表，等价于 SELECT table_a.id, table_b.id FROM table_b INNER JOIN table_a ON table_a.tb_id = table_b.id WHERE table_b.id &lt; 100 AND table_a.tb_id &lt; 100; 10. &lt;font face=&quot;华文新魏&quot; size=&quot;3&quot;&gt;列表IN()的比较&lt;/font&gt; MySql会对IN()里面的数据进行排序，然后用二分法查找某个值是否在列表中，这个算法的效率是O(Log n)。这其他数据库等价转换为多个OR条件连接的复杂度O(n)来说，IN()里大量取值时会更快。 ## MySQL如何关联查询 MySQL关联查询策略: 其实就是嵌套循环查询。MySQL先从第一个表循环读，然后再嵌套循环到下一个表寻找匹配，如此反复，直到找到所有表的匹配的行为止。 举个栗子： - 内连接sql SELECT tbl_user.name, tbl_bankcard.bankcardFROM tbl_user INNER JOIN tbl_bankcard ON tbl_user.id = tbl_bankcard.user_idWHERE tbl_user.moblie in (‘18611112222’,’18611113333’) MySQL在查询这条SQL时，用伪代码表示查询过程如下: ```ruby outer_iter = iterator over tbl_user where moblie in (&#39;18611112222&#39;,&#39;18611113333&#39;); outer_row = outer_iter.next; while outer_row: inner_iter = iterator over tbl_bankcard where user_id = outer_row.id; inner_row = inner_iter.next; while inner_row: output [outer_row.name, inner_row.bankcard] inner_row = inner_iter.next; end outer_row = outer_iter.next; end 再看这个SQL左外连接版本：SELECT tbl_user.name, tbl_bankcard.bankcard FROM tbl_user LEFT JOIN tbl_bankcard ON tbl_user.id = tbl_bankcard.user_id WHERE tbl_user.moblie in (&#39;18611112222&#39;,&#39;18611113333&#39;) MySQL在查询这条SQL时，用伪代码表示查询过程如下:outer_iter = iterator over tbl_user where moblie in (&#39;18611112222&#39;,&#39;18611113333&#39;); outer_row = outer_iter.next; while outer_row: inner_iter = iterator over tbl_bankcard where user_id = outer_row.id; inner_row = inner_iter.next; if inner_row: while inner_row: output [outer_row.name, inner_row.bankcard] inner_row = inner_iter.next; end else output [outer_row.name, NULL] end outer_row = outer_iter.next; end 基本上MySQL所有类型的查询都是这种方式运行。包括子查询，也是生成一张临时表，被当做普通表进行循环嵌套。以及右外链接也是会改写成等价的左外连接。 多表关联的一种方式： graph TD B(Join) --&gt; A[Join] C(Join) --&gt; A D(tbl1) --&gt; B E(tbl2) --&gt; B F(tbl3) --&gt; C G(tbl4) --&gt; C 但是MySQL是通过从一个表开始一直嵌套循环的方式： graph TD B(Join) --&gt; A[Join] C(tbl4) --&gt; A D[Join] --&gt; B E[tbl3] --&gt; B F[tbl1] --&gt; D G[tbl2] --&gt; D 关联查询优化器 MySQL查询优化器中最重要的一部分。它决定了多表查询的顺序。它评估不同顺序的成本选择成本最小的一个。 有时优化器给出的并不是最优的关联顺序，可使用STRAIGHT_JOIN关键字替换JOIN关键字重写查询。还是那句，“不要试着比优化器更聪明”。 不过，如果有超过n个表的关联，那么需要检查n的阶乘种关联顺序。我们称之为所有可能的执行计划的“搜索空间”。实际上，当需要关联的表超过 optimizer_search_depth 的限制的时候，就会选择“贪婪”搜索模式。 查询优化器的提示(hint)用于控制查询执行计划。列举一些课使用的提示： HIGH_PRIORITY 和 LOW_PRIORITY 当多条语句同时访问数据库时，设置语句优先级。HIGH_PRIORITY会使语句放在表的队列的最前面，LOW_PRIORITY则相反。这两个提示只对使用表锁的存储引擎有效。 DELAYED 这个提示用于INSERT和REPLACE。使用该提示会将插入的行数据放入缓冲区，然后在表空闲时批量写入数据。适合于日志插入等场景。但并不是所有存储引擎都支持，还会导致函数 LAST_INSERT_ID()无法正常工作。 STRAIGHT_JOIN 这个提示可用于SELECT语句中SELECT关键字之后，也可放置于两个关联表之间。该提示作用一是让查询中的表按语句出现的顺序关联。作用二是固定前后两个表的关联顺序。 SQL_SMALL_RESULT 和 SQL_BIG_RESULT 这个提示只对SELECT有效。它告诉优化器对GROUP BY或者DISTINCT查询如果使用临时表和排序。SQL_SMALL_RESULT会让优化器认为结果集很小，将结果放在内存中的索引临时表中，避免排序。SQL_BIG_RESULT 则告诉优化器结果集很大，在磁盘临时表进行排序。 SQL_BUFFER_RESULT 这个提示告诉优化器将结果放在临时表中，并且尽快释放掉表锁。 SQL_CACHE 和 SQL_NO_CACHE 这个提示告诉MySQL是否将结果集放在查询缓存中。 USING INDEX、IGNORE INDEX 和 FORCE INDEX 这几个提示分别告诉优化器 使用或者不使用或者强制使用索引。 控制优化器的一些参数SHOW VARIABLES LIKE &quot;optimizer_%&quot;; optimizer_search_depth控制穷举执行计划的限度。 optimizer_prune_level默认打开的，让优化器根据需要扫描的行数来决定是否跳过某些执行计划 optimizer_switch此变量包含了一些开关优化器特性的标志位。 总结 MySQL优化器做了很多工作把SQL语句变成更优的查询方式 使用 EXPLAIN EXTENDED … SHOW WARNINGS关键字查看优化后的指令 MySQL的关联查询是“嵌套循环”的 可使用查询优化提示控制查询执行计划，但是，“不要试着比优化器更聪明”。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dubbo暴露服务过程]]></title>
    <url>%2F2018%2F05%2F20%2FDubbo%E6%9A%B4%E9%9C%B2%E6%9C%8D%E5%8A%A1%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言dubbo框架也用了有一年了，一直没有详细的研究过dubbo源码。所以趁有时间好好学习体会dubbo的博大精深。本人才疏学浅，如有不对，请大神指点。这里使用的dubbo版本是2.6.1。 如何看源码？跟着Dubbo开发手册(中文)来喽。带着目的看源码，这次看dubbo是怎么暴露服务的。 先瞜一眼启动日志一般像这种大型的开源框架，都会有健全的启动日志，看看日志输出利于我们理解dubbo启动流程。日志输出从上往下看，dubbo做了哪些事： 暴露本地服务 暴露远程服务 启动Netty，绑定和暴露地址 连接zookeeper zookeeper订阅服务 监听zookeeper 先瞜一眼官方手册这段内容来自dubbo开发手册之实现细节 再来一段暴露服务时序图 接下来，从官方文档开始，分析dubbo服务暴露过程。 第一步， ServiceConfig分析前，先利用IDE生成类图看看ServiceConfig的继承关系。 问题一：这么多的配置是啥？凭借感觉像是和dubbo.xml里的配置属性有关系。先不管，留个坑。 根据时序图，我们先定位到 ServiceConfig 的 export()方法ServiceConfig#export public synchronized void export() { ... // 延迟暴露接口 if (delay != null &amp;&amp; delay &gt; 0) { delayExportExecutor.schedule(new Runnable() { public void run() { doExport(); } }, delay, TimeUnit.MILLISECONDS); } else { doExport(); // 此处调用开始暴露 } } 暴露服务是调用 ServiceConfig#doExport方法 protected synchronized void doExport() { if (unexported) { throw new IllegalStateException(&quot;Already unexported!&quot;); } if (exported) { return; } exported = true; if (interfaceName == null || interfaceName.length() == 0) { throw new IllegalStateException(&quot;&lt;dubbo:service interface=\&quot;\&quot; /&gt; interface not allow null!&quot;); } checkDefault();// 创建了 ProviderConfig 对象并赋值 setter is属性，提供者的缺省值设置 /** * provider已经配置的情况下，application、module、registries、monitor、protocol中未配置的值均可以从provider获取 */ if (provider != null) { if (application == null) { application = provider.getApplication(); } if (module == null) { module = provider.getModule(); } if (registries == null) { registries = provider.getRegistries(); } if (monitor == null) { monitor = provider.getMonitor(); } if (protocols == null) { protocols = provider.getProtocols(); } } if (module != null) { if (registries == null) { registries = module.getRegistries(); } if (monitor == null) { monitor = module.getMonitor(); } } if (application != null) { if (registries == null) { registries = application.getRegistries(); } if (monitor == null) { monitor = application.getMonitor(); } } if (ref instanceof GenericService) { interfaceClass = GenericService.class; if (StringUtils.isEmpty(generic)) { generic = Boolean.TRUE.toString(); } } else { try { interfaceClass = Class.forName(interfaceName, true, Thread.currentThread() .getContextClassLoader()); } catch (ClassNotFoundException e) { throw new IllegalStateException(e.getMessage(), e); } checkInterfaceAndMethods(interfaceClass, methods); // 检查配置中的 interface 属性 和 methods属性 checkRef(); // 检查 ref 属性 generic = Boolean.FALSE.toString(); } // 如果配置 local 属性， 是否服务接口客户端本地代理 if (local != null) { if (&quot;true&quot;.equals(local)) { local = interfaceName + &quot;Local&quot;; } Class&lt;?&gt; localClass; try { localClass = ClassHelper.forNameWithThreadContextClassLoader(local); } catch (ClassNotFoundException e) { throw new IllegalStateException(e.getMessage(), e); } if (!interfaceClass.isAssignableFrom(localClass)) { throw new IllegalStateException(&quot;The local implementation class &quot; + localClass.getName() + &quot; not implement interface &quot; + interfaceName); } } // 如果配置 stub 属性， 是否本地存根 if (stub != null) { if (&quot;true&quot;.equals(stub)) { stub = interfaceName + &quot;Stub&quot;; } Class&lt;?&gt; stubClass; try { stubClass = ClassHelper.forNameWithThreadContextClassLoader(stub); } catch (ClassNotFoundException e) { throw new IllegalStateException(e.getMessage(), e); } if (!interfaceClass.isAssignableFrom(stubClass)) { throw new IllegalStateException(&quot;The stub implementation class &quot; + stubClass.getName() + &quot; not implement interface &quot; + interfaceName); } } checkApplication(); // 检查 application 属性 checkRegistry(); // 检查 registry 属性 checkProtocol(); // 检查 protocol 属性 appendProperties(this); // 赋值 ServiceConfig setter is 属性 checkStubAndMock(interfaceClass); // 检查是否 使用 local,stub,mock 代理 if (path == null || path.length() == 0) { path = interfaceName; } doExportUrls(); // 开始暴露远程服务了 ProviderModel providerModel = new ProviderModel(getUniqueServiceName(), this, ref); ApplicationModel.initProviderModel(getUniqueServiceName(), providerModel); } ServiceConfig#doExportUrls暴露多个远程地址 private void doExportUrls() { // dubbo支持多注册中心，所以这一步把 registry 配置信息封装为多个url,比如 registry://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-provider... List&lt;URL&gt; registryURLs = loadRegistries(true); // dubbo是支持多协议的，将所有注册的url上对应的协议暴露出来 for (ProtocolConfig protocolConfig : protocols) { doExportUrlsFor1Protocol(protocolConfig, registryURLs); } } ServiceConfig#doExportUrlsFor1Protocol暴露单个地址 private void doExportUrlsFor1Protocol(ProtocolConfig protocolConfig, List&lt;URL&gt; registryURLs) { String name = protocolConfig.getName(); if (name == null || name.length() == 0) { name = &quot;dubbo&quot;; } // map存放所有配置参数，下面生成url用 Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put(Constants.SIDE_KEY, Constants.PROVIDER_SIDE); map.put(Constants.DUBBO_VERSION_KEY, Version.getVersion()); map.put(Constants.TIMESTAMP_KEY, String.valueOf(System.currentTimeMillis())); if (ConfigUtils.getPid() &gt; 0) { map.put(Constants.PID_KEY, String.valueOf(ConfigUtils.getPid())); } appendParameters(map, application); appendParameters(map, module); appendParameters(map, provider, Constants.DEFAULT_KEY); appendParameters(map, protocolConfig); appendParameters(map, this); // method子标签配置规则解析，暂时不管 if (methods != null &amp;&amp; !methods.isEmpty()) { for (MethodConfig method : methods) { ... } // end of methods for } // 获取所有方法添加到map中，体现在url里 if (ProtocolUtils.isGeneric(generic)) { // 如果是泛化实现，generic属性为true，method=*表示任意方法 map.put(&quot;generic&quot;, generic); map.put(&quot;methods&quot;, Constants.ANY_VALUE); } else { String revision = Version.getVersion(interfaceClass, version); if (revision != null &amp;&amp; revision.length() &gt; 0) { map.put(&quot;revision&quot;, revision); } String[] methods = Wrapper.getWrapper(interfaceClass).getMethodNames(); if (methods.length == 0) { logger.warn(&quot;NO method found in service interface &quot; + interfaceClass.getName()); map.put(&quot;methods&quot;, Constants.ANY_VALUE); } else { map.put(&quot;methods&quot;, StringUtils.join(new HashSet&lt;String&gt;(Arrays.asList(methods)), &quot;,&quot;)); } } // 如果配置了token属性，如果配为default则随机UUID，否则使用配置中的token，作令牌验证用 if (!ConfigUtils.isEmpty(token)) { if (ConfigUtils.isDefault(token)) { map.put(&quot;token&quot;, UUID.randomUUID().toString()); } else { map.put(&quot;token&quot;, token); } } // 如果协议是 injvm，就不注册服务， notify设置为false if (&quot;injvm&quot;.equals(protocolConfig.getName())) { protocolConfig.setRegister(false); map.put(&quot;notify&quot;, &quot;false&quot;); } // export service String contextPath = protocolConfig.getContextpath(); // 如果 protocol配置没有配置contextPath属性，就从provider配置中取 if ((contextPath == null || contextPath.length() == 0) &amp;&amp; provider != null) { contextPath = provider.getContextpath(); } String host = this.findConfigedHosts(protocolConfig, registryURLs, map); Integer port = this.findConfigedPorts(protocolConfig, name, map); // 根据上面的参数创建url对象 URL url = new URL(name, host, port, (contextPath == null || contextPath.length() == 0 ? &quot;&quot; : contextPath + &quot;/&quot;) + path, map); // 如果url使用的协议存在扩展，调用对应的扩展来修改原url。 if (ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class) .hasExtension(url.getProtocol())) { url = ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class) .getExtension(url.getProtocol()).getConfigurator(url).configure(url); } String scope = url.getParameter(Constants.SCOPE_KEY); // 如果scope属性没有配置为 none if (!Constants.SCOPE_NONE.toString().equalsIgnoreCase(scope)) { // 如果scope属性没有配置为 remote， 暴露本地服务 if (!Constants.SCOPE_REMOTE.toString().equalsIgnoreCase(scope)) { exportLocal(url); } // // 如果scope属性没有配置为 local， 暴露远程服务 if (!Constants.SCOPE_LOCAL.toString().equalsIgnoreCase(scope)) { if (logger.isInfoEnabled()) { logger.info(&quot;Export dubbo service &quot; + interfaceClass.getName() + &quot; to url &quot; + url); } if (registryURLs != null &amp;&amp; !registryURLs.isEmpty()) { for (URL registryURL : registryURLs) { url = url.addParameterIfAbsent(Constants.DYNAMIC_KEY, registryURL.getParameter(Constants.DYNAMIC_KEY)); URL monitorUrl = loadMonitor(registryURL); if (monitorUrl != null) { // 如果有monitor信息，则在url上增加monitor配置 url = url.addParameterAndEncoded(Constants.MONITOR_KEY, monitorUrl.toFullString()); } if (logger.isInfoEnabled()) { logger.info(&quot;Register dubbo service &quot; + interfaceClass.getName() + &quot; url &quot; + url + &quot; to registry &quot; + registryURL); } // 重要的第二步了，创建 invoker 对象（这里暴露远程协议里，在远程协议里增加了属性 export=url,url默认dubbo协议暴露地址） Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, registryURL.addParameterAndEncoded(Constants.EXPORT_KEY, url.toFullString())); DelegateProviderMetaDataInvoker wrapperInvoker = new DelegateProviderMetaDataInvoker(invoker, this); // 第三步，官方文档加重点的一步，invoker转化为 exporter Exporter&lt;?&gt; exporter = protocol.export(wrapperInvoker); exporters.add(exporter); } } else { Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, url); DelegateProviderMetaDataInvoker wrapperInvoker = new DelegateProviderMetaDataInvoker(invoker, this); Exporter&lt;?&gt; exporter = protocol.export(wrapperInvoker); exporters.add(exporter); } } } this.urls.add(url); } 第二步，ProxyFactory.getInvoker在ServiceConfig#doExportUrlsFor1Protocol暴露单个地址中的调用: Invoker&lt;?&gt; invoker = proxyFactory.getInvoker(ref, (Class) interfaceClass, registryURL.addParameterAndEncoded(Constants.EXPORT_KEY, url.toFullString())); 接下来看看这一行代码里做了什么。 问题二：这个 ProxyFactory$Adaptive是什么东东？看看 proxyFactory 是怎么来的。 private static final ProxyFactory proxyFactory = ExtensionLoader.getExtensionLoader(ProxyFactory.class).getAdaptiveExtension(); 看来是和这个 ExtensionLoader 有关。看看接口: @SPI(&quot;javassist&quot;) public interface ProxyFactory { @Adaptive({Constants.PROXY_KEY}) &lt;T&gt; T getProxy(Invoker&lt;T&gt; invoker) throws RpcException; @Adaptive({Constants.PROXY_KEY}) &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) throws RpcException; } @SPI 看起来和java SPI机制有关哦。先留个坑，回头再解决。 但是通过我们debug发现，默认情况下 ProxyFactory的实现是 JavassistProxyFactory。 public &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) { // TODO Wrapper cannot handle this scenario correctly: the classname contains &#39;$&#39; final Wrapper wrapper = Wrapper.getWrapper(proxy.getClass().getName().indexOf(&#39;$&#39;) &lt; 0 ? proxy.getClass() : type); return new AbstractProxyInvoker&lt;T&gt;(proxy, type, url) { @Override protected Object doInvoke(T proxy, String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Throwable { return wrapper.invokeMethod(proxy, methodName, parameterTypes, arguments); } }; } 正如官方文档所说: 首先 ServiceConfig 类拿到对外提供服务的实际类 ref(如：HelloWorldImpl),然后通过 ProxyFactory 类的 getInvoker 方法使用 ref 生成一个 AbstractProxyInvoker 实例，到这一步就完成具体服务到 Invoker 的转化。 JavassistProxyFactory的getInvoker实现是先创建一个包装类Wrapper ，包装类来实现远程调用。简单看下这个包装类是什么吧，Wapper.makeWrapper(Class&lt;?&gt; c):结果大致如下 public class Wrapper1 extends Wrapper { public static String[] pns; public static Map pts; public static String[] mns; // all method name array. public static String[] dmns; public static Class[] mts0; public String[] getPropertyNames() { return pns; } public boolean hasProperty(String n) { return pts.containsKey($1); } public Class getPropertyType(String n) { return (Class) pts.get($1); } public String[] getMethodNames() { return mns; } public String[] getDeclaredMethodNames() { return dmns; } public void setPropertyValue(Object o, String n, Object v) { dubbo.provider.hello.service.impl.HelloServiceImpl w; try { w = ((dubbo.provider.hello.service.impl.HelloServiceImpl) $1); } catch (Throwable e) { throw new IllegalArgumentException(e); } throw new com.alibaba.dubbo.common.bytecode.NoSuchPropertyException(&quot;Not found property \&quot;&quot; + $2 + &quot;\&quot; filed or setter method in class dubbo.provider.hello.service.impl.HelloServiceImpl.&quot;); } public Object getPropertyValue(Object o, String n) { dubbo.provider.hello.service.impl.HelloServiceImpl w; try { w = ((dubbo.provider.hello.service.impl.HelloServiceImpl) $1); } catch (Throwable e) { throw new IllegalArgumentException(e); } throw new com.alibaba.dubbo.common.bytecode.NoSuchPropertyException(&quot;Not found property \&quot;&quot; + $2 + &quot;\&quot; filed or setter method in class dubbo.provider.hello.service.impl.HelloServiceImpl.&quot;); } public Object invokeMethod(Object o, String n, Class[] p, Object[] v) throws java.lang.reflect.InvocationTargetException { dubbo.provider.hello.service.impl.HelloServiceImpl w; try { w = ((dubbo.provider.hello.service.impl.HelloServiceImpl) $1); } catch (Throwable e) { throw new IllegalArgumentException(e); } try { if (&quot;sayHello&quot;.equals($2) &amp;&amp; $3.length == 0) { w.sayHello(); return null; } } catch (Throwable e) { throw new java.lang.reflect.InvocationTargetException(e); } throw new com.alibaba.dubbo.common.bytecode.NoSuchMethodException(&quot;Not found method \&quot;&quot; + $2 + &quot;\&quot; in class dubbo.provider.hello.service.impl.HelloServiceImpl.&quot;); } } 第三步，invoker转化为 exporter在ServiceConfig#doExportUrlsFor1Protocol暴露单个地址中的调用: Exporter&lt;?&gt; exporter = protocol.export(wrapperInvoker); 由代码可知，这里的 protocol 和第二步里的proxyFactory 一样 private static final Protocol protocol = ExtensionLoader.getExtensionLoader(Protocol.class).getAdaptiveExtension(); 我们再看看 Protocol接口，也是SPI机制： @SPI(&quot;dubbo&quot;) public interface Protocol { int getDefaultPort(); @Adaptive &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException; @Adaptive &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException; void destroy(); } 凭感觉会使用dubbo协议调用到DubboProtocol。先debug,果不其然。看一眼调用栈： 从ServiceConfig之后，有两次协议调用，先是调用RegistryProtocol，然后RegistryProtocol里调用了DubboProtocol。两次暴露协议前，都会调用到 ProtocolListenerWrapper 和 ProtocolFilterWrapper，看看这两个地方。ProtocolListenerWrapper#export方法 public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException { // registry类型的Invoker，直接暴露 if (Constants.REGISTRY_PROTOCOL.equals(invoker.getUrl().getProtocol())) { return protocol.export(invoker); } // 非Registry类型的Invoker，需要被监听器包装 // 这里的protocol是ProtocolFilterWrapper return new ListenerExporterWrapper&lt;T&gt;(protocol.export(invoker), Collections.unmodifiableList(ExtensionLoader.getExtensionLoader(ExporterListener.class) .getActivateExtension(invoker.getUrl(), Constants.EXPORTER_LISTENER_KEY))); } ProtocolFilterWrapper#export方法 public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException { // registry类型的Invoker，直接暴露 if (Constants.REGISTRY_PROTOCOL.equals(invoker.getUrl().getProtocol())) { return protocol.export(invoker); } //非Registry类型的Invoker需要先构建调用链，然后再暴露 return protocol.export(buildInvokerChain(invoker, Constants.SERVICE_FILTER_KEY, Constants.PROVIDER)); } 这里构建调用链，控制invoker调用执行顺序，默认的filters如下图：暂且不谈Filter接口相关。 按照调用顺序，先调用 RegistryProtocol#export public &lt;T&gt; Exporter&lt;T&gt; export(final Invoker&lt;T&gt; originInvoker) throws RpcException { //export invoker final ExporterChangeableWrapper&lt;T&gt; exporter = doLocalExport(originInvoker); // 本地暴露 URL registryUrl = getRegistryUrl(originInvoker); // 获取注册地址，默认是dubbo，我这里使用zookeeper //registry provider final Registry registry = getRegistry(originInvoker); // 获取注册中心， 我这里的是ZookeeperRegistry对象 final URL registedProviderUrl = getRegistedProviderUrl(originInvoker);// 获取注册的提供者地址 //to judge to delay publish whether or not boolean register = registedProviderUrl.getParameter(&quot;register&quot;, true); ProviderConsumerRegTable.registerProvider(originInvoker, registryUrl, registedProviderUrl);// 注册提供者消费者。 if (register) { register(registryUrl, registedProviderUrl); // 注册服务 ProviderConsumerRegTable.getProviderWrapper(originInvoker).setReg(true); // 标记为已注册 } // Subscribe the override data // FIXME When the provider subscribes, it will affect the scene : a certain JVM exposes the service and call the same service. Because the subscribed is cached key with the name of the service, it causes the subscription information to cover. final URL overrideSubscribeUrl = getSubscribedOverrideUrl(registedProviderUrl); final OverrideListener overrideSubscribeListener = new OverrideListener(overrideSubscribeUrl, originInvoker); overrideListeners.put(overrideSubscribeUrl, overrideSubscribeListener); registry.subscribe(overrideSubscribeUrl, overrideSubscribeListener); //保证每次暴露服务返回一个新的exporter return new DestroyableExporter&lt;T&gt;(exporter, originInvoker, overrideSubscribeUrl, registedProviderUrl); } 然后，上面的本地暴露 doLocalExport(originInvoker) 实际上是暴露的dubbo协议，看下DubboProtocol： public &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException { URL url = invoker.getUrl(); // export service. String key = serviceKey(url); // 获取dubbo协议服务key，serviceGroup/serviceName:serviceVersion:port DubboExporter&lt;T&gt; exporter = new DubboExporter&lt;T&gt;(invoker, key, exporterMap); exporterMap.put(key, exporter); //export an stub service for dispatching event Boolean isStubSupportEvent = url.getParameter(Constants.STUB_EVENT_KEY, Constants.DEFAULT_STUB_EVENT); // 是否是stub事件？ dubbo.stub.event属性 Boolean isCallbackservice = url.getParameter(Constants.IS_CALLBACK_SERVICE, false); // 是否回调服务？ is_callback_service属性 if (isStubSupportEvent &amp;&amp; !isCallbackservice) { // TODO 这里暂时不分析，此处demo场景为false String stubServiceMethods = url.getParameter(Constants.STUB_EVENT_METHODS_KEY); if (stubServiceMethods == null || stubServiceMethods.length() == 0) { if (logger.isWarnEnabled()) { logger.warn(new IllegalStateException(&quot;consumer [&quot; + url.getParameter(Constants.INTERFACE_KEY) + &quot;], has set stubproxy support event ,but no stub methods founded.&quot;)); } } else { stubServiceMethodsMap.put(url.getServiceKey(), stubServiceMethods); } } openServer(url); // 开启服务(这里默认使用netty方式) optimizeSerialization(url);// 优化序列化 return exporter; } 至此返回 exporter 之后，就完成了 invoker到exporter的转化。返回到ServiceConfig后服务发布过程到此结束。 补充先看看之前遗留的两个问题：问题一：AbstractConfig 衍生的子类(ServiceConfig,ProviderConfig,RegistryConfig等) ，这么多的配置类是啥？ 问题二：这个 ProxyFactory$Adaptive是什么东东？ 看看 proxyFactory 是怎么来的。 回答问题一：在dubbo-config模块中，代码里的解释已经很清楚了。这里简单介绍几个抽象配置: AbstractConfig：配置模板，配置解析的工具方法、公共方法，提供几个主要的方法（appendAnnotation，appendProperties，appendParameters，appendAttributes等）。 AbstractMethodConfig：封装了一些方法级别的相关属性 AbstractInterfaceConfig：封装了接口需要的属性 AbstractReferenceConfig：主要是引用实例的配置 再看一下dubbo-config-spring模块，与spring如何整合的：spring.handlers文件里如是写道： http\://code.alibabatech.com/schema/dubbo=com.alibaba.dubbo.config.spring.schema.DubboNamespaceHandler dubbo的schema标签的定义就在DubboNamespaceHandler类中: public class DubboNamespaceHandler extends NamespaceHandlerSupport { static { Version.checkDuplicate(DubboNamespaceHandler.class); } public void init() { registerBeanDefinitionParser(&quot;application&quot;, new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser(&quot;module&quot;, new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser(&quot;registry&quot;, new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser(&quot;monitor&quot;, new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser(&quot;provider&quot;, new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser(&quot;consumer&quot;, new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser(&quot;protocol&quot;, new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser(&quot;service&quot;, new DubboBeanDefinitionParser(ServiceBean.class, true)); registerBeanDefinitionParser(&quot;reference&quot;, new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser(&quot;annotation&quot;, new AnnotationBeanDefinitionParser()); } } 所以，一目了然。 回答问题二：这确实和dubbo插件机制有关，后面再单独写一篇文章分析。 提出问题三:dubbo怎么启动的？本文的分析是直接从dubbo文档和启动日志起手的，那么dubbo是怎么从加载spring容器到ServiceConfig暴露服务的呢？ 再回顾看下dubbo暴露服务调用栈：还记得上面的 AbstractConfig家族的类图吗。ServiceBean继承自ServiceConfig。ServiceBean实现了pplicationListener接口，实现方法: public void onApplicationEvent(ContextRefreshedEvent event) { if (isDelay() &amp;&amp; !isExported() &amp;&amp; !isUnexported()) { if (logger.isInfoEnabled()) { logger.info(&quot;The service ready on spring started. service: &quot; + getInterface()); } export(); // 调用父类ServiceConfig的export() } } 可见，在spring容器实例化bean完成后，发布ContextRefreshedEvent事件时调用ServiceConfig的export()方法。看看日志是不是有“The service ready on spring started. service:xxx”且在服务暴露日志前呢~ 后言至此，初步完成了dubbo服务暴露过程的解析（ServiceConfig-&gt; Invoker-&gt;Exporter），但是上面服务暴露过程有些内容并没有详细分析，比如 本地暴露与远程暴露的细枝末节 dubbo的扩展机制 获取注册中心注册服务（zookeeper）的过程 开启服务过程(Netty服务)… 这些后面一点点剖析。 总结 dubbo暴露服务过程总体分为三步：ServiceConfig-&gt; Invoker-&gt;Exporter. AbstractConfig家族是spring与dubbo整合的核心配置 ServiceBean中开启spring容器加载完成后的暴露服务过程]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Bean Validation 解决业务中参数校验]]></title>
    <url>%2F2018%2F05%2F10%2F%E4%BD%BF%E7%94%A8%20Bean%20Validation%20%E8%A7%A3%E5%86%B3%E4%B8%9A%E5%8A%A1%E4%B8%AD%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[目的精简业务中的代码校验。 痛点及现状代码中常常见到如下代码： if (Objects.equal(0L ,repertory)){ return ApiResultMap.errorResult(-1 ,&quot;操作数量不可为0&quot;) ; } 这种参数校验写在模块里有如下缺点： 代码冗余 影响代码可读性 需要通过注释来知道每个入参的约束是什么。 每个程序员做参数验证的方式不一样，参数验证不通过抛出的异常也不一样。 抛出问题：那么有没有一种方式可以简化代码呢？ JSR 303 - Bean ValidationBean Validation是一个通过配置注解来验证参数的框架，它包含两部分Bean Validation API和Hibernate Validator。 Bean Validation API是Java定义的一个验证参数的规范。 Hibernate Validator是Bean Validation API的一个实现。 QUICK START 引入pom &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;5.3.1.Final&lt;/version&gt; &lt;/dependency&gt; dto入参对象属性加入注解`@Datapublic class ValidDemo { @Size(min = 3, max = 12, message = “用户名必须的长度必须是3到12个字母之间”) @Pattern(regexp = “^[a-z]+$”, message = “用户名必须是a-z小字母”) private String name; @Size(min = 6, max = 6, message = “密码必须是6位数字”) @Pattern(regexp = “^[0-9]+$”, message = “密码必须是6位数字”) private String password; @Range(min = 1, max = 9, message = “范围只能1到9”) private Integer range; @NotNull(message = “邮箱不能为Null”) @Email(regexp = “(?:[a-z0-9!#$%&amp;’+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&amp;’+/=?^_`{|}~-]+)|\”(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])\”)@(?:(?:a-z0-9?\.)+a-z0-9?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])”, message = &quot;邮箱格式有误&quot;) private String email; } 3. controller方法入参加入校验(@Valid) @GetMapping(“/validdemo”)public Map&lt;String,Object&gt; demo(@Valid ValidDemo validDemo){ return ApiResultMap.successResult(validDemo);} 4. 精简出参，加入全局异常处理 @ExceptionHandler(value = { BindException.class })public Map&lt;String, Object&gt; validationException(BindException ex) { log.error(ex.getBindingResult().getFieldError().getDefaultMessage()); return ApiResultMap.errorResult(ex.getBindingResult().getFieldError().getDefaultMessage());} 5. 测试结果： $curl http://localhost:8080/validdemo?email=xxxxx { “message”: { “code”: -1, “message”: “邮箱格式有误” }} **另一种方式，使用 spring 的 @Validated 注解:** 1. 配置 MethodValidationPostProcessor @Bean public MethodValidationPostProcessor methodValidationPostProcessor() { return new MethodValidationPostProcessor(); } 2. 使用@Validated注解： @Validated@RestControllerpublic class DemoController {} 3. 方法上加上校验 @GetMapping(“/validdemo3”)public Map&lt;String,Object&gt; demo3(@NotNull String str, @NotNull @Range(min = 0, max = 10) Integer a){ return ApiResultMap.successResult(str + a);} 4. 测试 $curl http://localhost:8080/validdemo?str=1&amp;a=15{ “message”: { “code”: 202, “message”: “需要在0和10之间” }}` 应用场景java程序员张三和ios程序员李四开发某一需求，明天就要demo了，今天得抓紧联调。李四:三哥，地址发下，调试绑卡。张三启动服务…李四：三哥，帮忙看下，这个接口报错 -1，看下上面错误呗。张三打开控制台，看了一下日志与排查，数据库报错，身份证字段没传导致插入身份证为空报错。张三：李四你身份证没传。…张三一边埋怨着一边加入一行if else，同时在思考别的接口是不是也有这种情况，也给加上if else.没过一会儿，李四：三哥，帮我看下有报错-1了。张三又去查看，同样的，这次是银行卡号没传。张三重复着以上操作可没过一会儿李四又….就这样一天过去，伴随着晚霞下班的张三，心情却没那么高兴… 抛出问题：是什么导致张三忙碌一天却觉得碌碌无为？是道德的沦丧还是人性的丧失？ 如果张三整合Bean Validation的话，可能就没有那么不愉快了。他只需要一次在入参model里加入校验，之后在控制器的通过轻松的 @Valid 注解，就可以省去李四重复的提问，也省去的检查其他接口是否也需要添加代码校验，代码又可以少些几行了，何乐而不为。 常用注解Bean Validation 中内置的 constraint： @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max, min) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(value) 被注释的元素必须符合指定的正则表达式 Hibernate Validator 附加的 constraint： @Email 被注释的元素必须是电子邮箱地址 @Length 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range 被注释的元素必须在合适的范围内 优势 代码整洁 代码可读性强 解决不同开发者的不同的校验方式 总结推荐使用 Bean Validation 的方式解决业务中参数校验；这里只给出了一些基本的参数校验constraint，在实际业务中可根据业务情形自定义业务constraint。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA神器居然还有这些小技巧]]></title>
    <url>%2F2018%2F05%2F08%2FIntellij%20IDEA%E7%A5%9E%E5%99%A8%E5%B1%85%E7%84%B6%E8%BF%98%E6%9C%89%E8%BF%99%E4%BA%9B%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[前言Intellij IDEA，一个开发者甚爱的IDE，用了这么多年，其中有些小Tips你真的了解吗？ 本文转载自Sam哥哥聊技术的技术博文。 原文地址:https://blog.csdn.net/linsongbin1/article/details/80211919 概述Intellij IDEA真是越用越觉得它强大，它总是在我们写代码的时候，不时给我们来个小惊喜。出于对Intellij IDEA的喜爱，我决定写一个与其相关的专栏或者系列，把一些好用的Intellij IDEA技巧分享给大家。本文是这个系列的第一篇，主要介绍一些你可能不知道的但是又实用的小技巧。 我最爱的【演出模式】我们可以使用【Presentation Mode】，将IDEA弄到最大，可以让你只关注一个类里面的代码，进行毫无干扰的coding。 可以使用Alt+V快捷键，弹出View视图，然后选择Enter Presentation Mode。效果如下： 这个模式的好处就是，可以让你更加专注，因为你只能看到特定某个类的代码。可能读者会问，进入这个模式后，我想看其他类的代码怎么办？这个时候，就要考验你快捷键的熟练程度了。你可以使用CTRL+E弹出最近使用的文件。又或者使用CTRL+N和CTRL+SHIFT+N定位文件。 如何退出这个模式呢？很简单，使用ALT+V弹出view视图，然后选择Exit Presentation Mode 即可。但是我强烈建议你不要这么做，因为你是可以在Enter Presentation Mode模式下在IDEA里面做任何事情的。当然前提是，你对IDEA足够熟练。 神奇的Inject language如果你使用IDEA在编写JSON字符串的时候，然后要一个一个\去转义双引号的话，就实在太不应该了，又烦又容易出错。在IDEA可以使用Inject language帮我们自动转义双引号。 先将焦点定位到双引号里面，使用alt+enter快捷键弹出inject language视图，并选中Inject language or reference。 选择后,切记，要直接按下enter回车键，才能弹出inject language列表。在列表中选择json组件。 选择完后。鼠标焦点自动会定位在双引号里面，这个时候你再次使用alt+enter就可以看到 选中Edit JSON Fragment并回车，就可以看到编辑JSON文件的视图了。 可以看到IDEA确实帮我们自动转义双引号了。如果要退出编辑JSON信息的视图，只需要使用ctrl+F4快捷键即可。 Inject language可以支持的语言和操作多到你难以想象，读者可以自行研究。 使用快捷键移动分割线假设有下面的场景，某个类的名字在project视图里被挡住了某一部分。 你想完整的看到类的名字，该怎么做。一般都是使用鼠标来移动分割线，但是这样子效率太低了。可以使用alt+1把鼠标焦点定位到project视图里，然后直接使用ctrl+shift+左右箭头来移动分割线。 ctrl+shift+enter不只是用来行尾加分号的ctrl+shift+enter其实是表示为您收尾的意思，不只是用来给代码加分号的。比如说： 这段代码，我们还需要为if语句加上大括号才能编译通过，这个时候你直接输入ctrl+shift+enter，IDEA会自动帮你收尾，加上大括号的。 不要动不动就使用IDEA的重构功能IDEA的重构功能非常强大，但是也有时候，在单个类里面，如果只是想批量修改某个文本，大可不必使用到重构的功能。比如说： 上面的代码中，有5个地方用到了rabbitTemplate文本，如何批量修改呢？首先是使用ctrl+w选中rabbitTemplate这个文本,然后依次使用5次alt+j快捷键，逐个选中，这样五个文本就都被选中并且高亮起来了，这个时候就可以直接批量修改了。 去掉导航栏去掉导航栏，因为平时用的不多。 可以把红色的导航栏去掉，让IDEA显得更加干净整洁一些。使用alt+v，然后去掉Navigation bar即可。去掉这个导航栏后，如果你偶尔还是要用的，直接用alt+home就可以临时把导航栏显示出来。 如果想让这个临时的导航栏消失的话，直接使用esc快捷键即可。 把鼠标定位到project视图里当工程里的包和类非常多的时候，有时候我们想知道当前类在project视图里是处在哪个位置。 上面图中的DemoIDEA里，你如何知道它是在spring-cloud-config工程里的哪个位置呢？可以先使用alt+F1，弹出Select in视图，然后选择Project View中的Project，回车，就可以立刻定位到类的位置了。 那如何从project跳回代码里呢？可以直接使用esc退出project视图，或者直接使用F4,跳到代码里。 强大的symbol如果你依稀记得某个方法名字几个字母，想在IDEA里面找出来，可以怎么做呢？直接使用ctrl+shift+alt+n，使用symbol来查找即可。比如说： 你想找到checkUser方法。直接输入user即可。 如果你记得某个业务类里面有某个方法，那也可以使用首字母找到类,然后加个 . ，再输入方法名字也是可以的。 如何找目录使用ctrl+shift+n后，使用 /，然后输入目录名字即可。 自动生成not null判断语句自动生成not null这种if判断，在IDEA里有很多种办法，其中一种办法你可能没想到。 当我们使用rabbitTemplate. 后，直接输入notnull并回车，IDEA就好自动生成if判断了。 按照模板找内容这个也是我非常喜欢的一个功能，可以根据模板来找到与模板匹配的代码块。比如说： 想在整个工程里面找到所有的try catch语句,但是catch语句里面没有做异常处理的。 catch语句里没有处理异常，是极其危险的。我们可以IDEA里面方便找到所有这样的代码。 首先使用ctrl+shift+A快捷键弹出action框，然后输入Search Struct 。 选择Search Structurally后，回车，跳转到模板视图。 点击Existing Templates按钮，选择try模板。为了能找出catch里面没有处理异常的代码块，我们需要配置一下CatchStatement的Maximum count的值，将其设置为1。 点击Edit Variables按钮，在界面修改Maximum count的值。 最后点击find按钮，就可以找出catch里面没有处理异常的代码了。 结束语感谢 Sam哥哥聊技术 的分享！ 此篇文章是收藏级别的文章，多学多用，才能使coding效率大大的提高。]]></content>
      <categories>
        <category>Intellij IDEA</category>
      </categories>
      <tags>
        <tag>转载摘抄</tag>
        <tag>Intellij IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu自定义安装Mysql全记录]]></title>
    <url>%2F2018%2F04%2F22%2Fubuntu%E8%87%AA%E5%AE%9A%E4%B9%89%E5%AE%89%E8%A3%85Mysql%E5%85%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言记录一下MySQL 5.7版本的安装过程。 MySQL安装零. 参考：官方文档 一. 下载mysql二进制文件，进入页面： https://dev.mysql.com/downloads/mysql/Select Operating System: Linux-Generic并下载 二. 切换到root用户创建group和user root@chenruiwen:/data/mysql# groupadd mysql root@chenruiwen:/data/mysql# useradd -r -g mysql dba -s /bin/false mysql 三. 解压下载的Mysql,重命名 root@chenruiwen:/data/mysql# tar -xf mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz root@chenruiwen:/data/mysql# mv mysql-5.7.21-linux-glibc2.12-x86_64 mysql-5.7.21 四. 赋予mysql操作权限 root@chenruiwen:/data/mysql/mysql-5.7.21# chown -R dba /data/mysql/mysql-5.7.21 root@chenruiwen:/data/mysql/mysql-5.7.21# chgrp -R mysql /data/mysql/mysql-5.7.21 root@chenruiwen:/data/mysql/mysql-5.7.21# mkdir mysql-files root@chenruiwen:/data/mysql/mysql-5.7.21# chown dba:mysql mysql-files/ root@chenruiwen:/data/mysql/mysql-5.7.21# chmod 750 mysql-files 五. 初始化Mysql root@chenruiwen:/data/mysql/mysql-5.7.21# bin/mysqld --initialize --user=dba mysqld: Can&#39;t create directory &#39;/usr/local/mysql/data/&#39; (Errcode: 2 - No such file or directory) 2018-04-08T15:25:22.856110Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2018-04-08T15:25:22.856195Z 0 [ERROR] Can&#39;t find error-message file &#39;/usr/local/mysql/share/errmsg.sys&#39;. Check error-message file location and &#39;lc-messages-dir&#39; configuration directive. 2018-04-08T15:25:22.856935Z 0 [ERROR] Aborting root@chenruiwen:/data/mysql/mysql-5.7.21# 提示目录不存在，因为我的mysql目录是在/data/mysql/mysql-5.7.21 所以，要指定数据目录和basedir并初始化。 root@chenruiwen:/data/mysql/mysql-5.7.21# mkdir data root@chenruiwen:/data/mysql/mysql-5.7.21# bin/mysqld --user=dba --basedir=/data/mysql/mysql-5.7.21 --datadir=/data/mysql/mysql-5.7.21/data/ --initialize-insecure 2018-04-28T03:12:52.827802Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details). 2018-04-28T03:12:53.883717Z 0 [Warning] InnoDB: New log files created, LSN=45790 2018-04-28T03:12:54.084754Z 0 [Warning] InnoDB: Creating foreign key constraint system tables. 2018-04-28T03:12:54.150087Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 0aca257d-4a92-11e8-af3b-00163e089ff5. 2018-04-28T03:12:54.153185Z 0 [Warning] Gtid table is not ready to be used. Table &#39;mysql.gtid_executed&#39; cannot be opened. 2018-04-28T03:12:54.153653Z 1 [Warning] root@localhost is created with an empty password ! Please consider switching off the --initialize-insecure option. 这里我使用了 –initialize-insecure 参数，不设置root密码。如果是 –initialize ，会随机生成一个密码。 六. 开启SSL，生成密钥文件 root@chenruiwen:/data/mysql/mysql-5.7.21# bin/mysql_ssl_rsa_setup 2018-04-28 11:21:57 [ERROR] Failed to access directory pointed by --datadir. Please make sure that directory exists and is accessible by mysql_ssl_rsa_setup. Supplied value : /usr/local/mysql/data 报错,需要修改配置文件。修改 /etc/my.cnf –datadir 属性,创建 my.cnf root@chenruiwen:/data/mysql/mysql-5.7.21# vi /etc/my.cnf MySQL配置文件简单版： [mysqld] # GENERAL character-set-server=utf8 symbolic-links=0 basedir=/data/mysql/mysql-5.7.21 datadir=/data/mysql/mysql-5.7.21/data socket=/tmp/mysql.sock user=dba port=3306 pid-file=/data/mysql/mysql-5.7.21/data/mysql.pid default_storage_engine=InnoDB #LOGGING log_error=/data/mysql/mysql-5.7.21/log/mysql-error.log #OTHER open_files_limit=65535 [client] socket=/tmp/mysql.sock port=3306 [mysqld_safe] log-error=/data/mysql/mysql-5.7.21/log/mysql-error.log pid-file=/data/mysql/mysql-5.7.21/data/mysql.pid 定义的日志目录和文件手工创建一下，然后重新运行： root@chenruiwen:/data/mysql/mysql-5.7.21# bin/mysql_ssl_rsa_setup Generating a 2048 bit RSA private key ............................................+++ ........................+++ writing new private key to &#39;ca-key.pem&#39; ----- Generating a 2048 bit RSA private key ....................................+++ .................+++ writing new private key to &#39;server-key.pem&#39; ----- Generating a 2048 bit RSA private key ..............................................................................+++ ..........................................+++ writing new private key to &#39;client-key.pem&#39; ----- root@chenruiwen:/data/mysql/mysql-5.7.21# 更多开启SSL相关请参考官方文档:Creating SSL and RSA Certificates and Keys七. 安全启动MySQL服务 root@chenruiwen:/data/mysql/mysql-5.7.21# bin/mysqld_safe --user=dba &amp; 查看MySQL服务进程 root@chenruiwen:/data/mysql/mysql-5.7.21# ps -ef|grep mysql root 14899 11849 0 15:20 pts/0 00:00:00 /bin/sh bin/mysqld_safe --user=dba dba 15135 14899 0 15:20 pts/0 00:00:00 /data/mysql/mysql-5.7.21/bin/mysqld --basedir=/data/mysql/mysql-5.7.21 --datadir=/data/mysql/mysql-5.7.21/data --plugin-dir=/data/mysql/mysql-5.7.21/lib/plugin --user=dba --log-error=/data/mysql/mysql-5.7.21/log/mysql-error.log --open-files-limit=65535 --pid-file=/data/mysql/mysql-5.7.21/data/mysql.pid --socket=/tmp/mysql.sock --port=3306 root 15201 11849 0 15:27 pts/0 00:00:00 grep --color=auto mysql 启动成功。 八. (可选项)加到系统服务 root@chenruiwen:/data/mysql/mysql-5.7.21# cp support-files/mysql.server /etc/init.d/mysql.server 修改 mysqld 文件里的 basedir，datadir， mysqld_pid_file_path属性 连接MySQLroot@chenruiwen:/data/mysql/mysql-5.7.21# mysql -uroot -p The program &#39;mysql&#39; can be found in the following packages: * mysql-client-core-5.7 * mariadb-client-core-10.0 Try: apt install &lt;selected package&gt; 安装客户端 root@chenruiwen:/data/mysql/mysql-5.7.21# apt-get update root@chenruiwen:/data/mysql/mysql-5.7.21# apt-get install mysql-client-core-5.7 安装完成后重新连接MySQL，输入密码时直接回车，登录成功。 修改 root 密码： mysql&gt; use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; update user set authentication_string = password(&#39;123456&#39;), password_expired = &#39;N&#39;, password_last_changed = now() where user = &#39;root&#39;; Query OK, 1 row affected, 1 warning (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 1 退出，重启即可。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8实战学习总结]]></title>
    <url>%2F2018%2F04%2F21%2Fjava8%E5%AE%9E%E6%88%98%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言java8作为一次大的升级，语法层面上有很大的改变，于是，来尝尝Java8的鲜。(ps: 哈哈，java10都出了，java8还不熟咋办呢~ 学学吧~) 为什么要用Java8 elasticsearch: Elasticsearch requires Java 8 or later. Use the official Oracle distribution or an open-source distribution such as OpenJDK. dubbo: Requires JDK1.8+, if you use lower version, see 1.6+, use 2.5.5 spring: JDK 8+ for Spring Framework 5.x…… 很多主流框架已经使用java8进行升级开发，java8是趋势，是时候一起拥抱java8了. java8语言新特性 函数式编程 Lambda表达式 接口的默认方法和静态方法 stream API 新的类库：Optional,Streams,Date/Time API (JSR 310)… JVM新特性 1.Lambda表达式Lambda允许把函数作为一个方法的参数（函数作为参数传递进方法中），或者把代码看成数据。 1.1 lambda的基本语法是：(parameters) -&gt; expression 或者 (parameters) -&gt; { statements; } eg:java8中有效的lambda表达式： 一个String类型的参数并返回一个int。Lambda中没有return语句，因为已经隐含了return。 (String s) -&gt; s.length() 两个int类型的参数而没有返回值。 (int x, int y) -&gt; { System.out.println(&quot;Result:&quot;); System.out.println(x+y); } 没有参数，返回一个int() -&gt; 1 1.2 函数式接口 函数式接口(函数式接口就是只定义一个抽象方法的接口) 函数描述符(函数式接口的抽象方法的签名 就是 lambda表达式的签名) java8中接口的变化:Java 8用默认方法与静态方法这两个新概念来扩展接口的声明。此功能是为了向后兼容性增加，使旧接口可用于利用JAVA8，lambda表达式的能力默认方法与抽象方法不同之处在于抽象方法必须要求实现，但是默认方法则没有这个要求。 在实际使用过程中，函数式接口是容易出错的：如有某个人在接口定义中增加了另一个方法，这时，这个接口就不再是函数式的了，并且编译过程也会失败。为了克服函数式接口的这种脆弱性并且能够明确声明接口作为函数式接口的意图，Java 8增加了一种特殊的注解@FunctionalInterface（Java 8中所有类库的已有接口都添加了@FunctionalInterface注解）。 目前已存在的函数式接口有：Comparable、Runnable和Callable等。java8在java.util.function中有引入了很多个新的函数式接口: Predicate 输入参数为类型T， 输出为类型boolean， 记作 T -&gt; boolean Consumer 输入参数为类型T， 输出为void， 记作 T -&gt; void Function&lt;T,R&gt; 输入参数为类型T， 输出为类型R， 记作 T -&gt; R Supplier 没有输入参数， 输出为类型T， 记作 void -&gt; T… 1.3方法引用方法引用让你可以重复使用现有的方法定义，并像Lambda一样传递它们。当你需要使用方法引用时，目标引用放在分隔符::前，方法的名称放在后面。 三种方法引用： 指向静态方法的方法引用。lambda表达式: (args) -&gt; ClassName.staticMethod(args)方法引用: ClassName::staticMethod 比如 s -&gt; Integer.valueOf(s) 等价于 Integer::valueOf 指向任意类型实例方法的方法引用。lambda表达式: (arg0, rest) -&gt; arg0.instanceMethod(rest)方法引用: ClassName::instanceMethod(上面args0是ClassName类型的) 比如 (str, integer) -&gt; str.substring(integer) 等价于 String::substring 指向现有对象的实例方法的方法引用。lambda表达式: (args) -&gt; expr.instanceMethod(args)方法引用: expr::instanceMethod 比如 (Apple arg) -&gt; appleExpr.compareAppleWight(arg) 等价于 appleExpr::compareAppleWight 构造函数的方法引用。对于一个现有构造函数，你可以利用它的名称和关键字new来创建它的一个引用：ClassName::new。它的功能与指向静态方法的引用类似。 比如创建一个Apple对象。空参构造器Apple()： Supplier&lt;Apple&gt; c1 = Apple::new; Apple a1 = c1.get(); 等价于 Supplier&lt;Apple&gt; c1 = () -&gt; new Apple(); Apple a1 = c1.get(); 两个参数的构造函数Apple(String color, Integer weight)： BiFunction&lt;Integer, String, Apple&gt; biFunction = Apple::new; Apple apple2 = biFunction.apply(155, &quot;green&quot;); 等价于 BiFunction&lt;Integer, String, Apple&gt; biFunction = (weight, color) -&gt; new Apple(weight, color); Apple apple2 = biFunction.apply(155, &quot;green&quot;); 1.4 复合lambda表达式 比较器(Comparator)复合(reversed(),thenComparing()) 谓词(Predicate)复合(negate(),and(),or()) 函数(Function&lt;T, V&gt;)复合(compose(),andThen(),identity()) 2.Stream APIJava 8 中的 Stream 是对集合（Collection）对象功能的增强，它专注于对集合对象进行各种非常便利、高效的聚合操作（aggregate operation），或者大批量数据操作 (bulk data operation)。 Stream API(java.util.stream.*)可以让你的代码具备： 声明性：更简洁，更易读 可复合：更灵活 可并行：性能更好 2.1 什么是流流不是一种数据结构，而是处理集合元素的相关计算，更像一个高级的 Iterator。单向，不可往复，数据只能遍历一次。 如何使用流？ 一个数据源（如集合）来执行一个查询； 一个中间操作链，形成一条流的流水线； 一个终端操作，执行流水线，并能生成结果。 2.2 使用流 筛选、切片和匹配 查找、匹配和归约 使用数值范围等数值流 从多个源创建流 无限流 2.2.1 筛选，切片 filter (筛选过滤) distinct (去重) limit (截取) skip (跳过) 2.2.2 映射 map (映射:接受一个函数作为参数。这个函数会被应用到每个元素上，并将其映射成一个新的元素) flatMap (flatmap方法让你把一个流中的每个值都换成另一个流，然后把所有的流连接起来成为一个流。) 2.2.3 查找和匹配 anyMatch (检查谓词是否至少匹配一个元素) allMatch (检查谓词是否匹配所有元素) noneMatch (检查是否没有任何元素与给定的谓词匹配) findAny (返回当前流中的任意元素Optional。它可以与其他流操作结合使用。) findFirst (返回流中第一个元素Optional) 2.2.4 reduce这个方法的主要作用是把 Stream 元素组合起来。它提供一个起始值（种子），然后依照运算规则（BinaryOperator），和前面 Stream 的第一个、第二个、第 n 个元素组合。从这个意义上说，字符串拼接、数值的 sum、min、max、average 都是特殊的 reduce。 Optional reduce(BinaryOperator accumulator); T reduce(T identity, BinaryOperator accumulator); U reduce(U identity,BiFunction&lt;U, ? super T, U&gt; accumulator,BinaryOperator&lt; U &gt; combiner); 2.2.5 数值流 IntStream DoubleStream LongStream 2.2.6 创建流 由值创建流(Stream.of) 由数组创建流(Arrays.stream(array)) 由文件生成流(java.nio.file.Files) 由函数生成流：创建无限流(Stream.iterate和Stream.generate) 2.3 收集器 用Collectors类创建和使用收集器 将数据流归约为一个值 汇总：归约的特殊情况 数据分组和分区 自定义收集器 2.3.1 归约和汇总Stream.reduce 与 Stream.collect的区别：Stream.reduce，常用的方法有average, sum, min, max, and count，返回单个的结果值，并且reduce操作每处理一个元素总是创建一个新值。Stream.collect修改现存的值，而不是每处理一个元素，创建一个新值。 2.3.2 数据分组 一级分组Map&lt;Dish.Type, List&lt;Dish&gt;&gt; groupByType = menuList.stream().collect(groupingBy(Dish::getType)); 多级分组Map&lt;Dish.Type, Map&lt;CaloricLevel, List&lt;Dish&gt;&gt;&gt; groupByTypeAndCalories = menuList.stream().collect( groupingBy(Dish::getType, groupingBy(dish -&gt; { if (dish.getCalories() &lt;= 400) return CaloricLevel.DIET; else if (dish.getCalories() &lt;= 700) return CaloricLevel.NORMAL; else return CaloricLevel.FAT; })) ); 按子组收集数据Map&lt;Dish.Type, Long&gt; groupByTypeToCount = menuList.stream().collect(groupingBy(Dish::getType, counting())); 分区(分区是一种特殊的分组，结果map至少包含两个不同的分组——一个true，一个false。)Map&lt;Boolean, List&lt;Dish&gt;&gt; partitionByVegeterian = menuList.stream().collect(partitioningBy(Dish::isVegetarian)); 2.3.3 Collector接口 public interface Collector&lt;T, A, R&gt; { Supplier&lt;A&gt; supplier() BiConsumer&lt;A, T&gt; accumulator() Function&lt;A, R&gt; finisher() BinaryOperator&lt;A&gt; combiner() Set&lt;Characteristics&gt; characteristics() } Collector接口的三个泛型： T：stream在调用collect方法收集前的数据类型 A：A是T的累加器，遍历T的时候，会把T按照一定的方式添加到A中，换句话说就是把一些T通过一种方式变成A R：R可以看成是A的累加器，是最终的结果，是把A汇聚之后的数据类型，换句话说就是把一些A通过一种方式变成R 通过自定义ToList收集器理解接口方法： Supplier supplier()怎么创建一个累加器（这里对应的是如何创建一个List） BiConsumer&lt;A, T&gt; accumulator()怎么把一个对象添加到累加器中（这里对应的是如何在List里添加一个对象，当然是调用add方法） Function&lt;A, R&gt; finisher()其实就是怎么把A转化为R，由于是toList，所以A和R是一样的类型，这里其实用就是Function.identity BinaryOperator combiner()它定义了对流的各个子部分进行并行处理时，各个子部分归约所得的累加器要如何合并（这里对应的是如何把List和List合并起来，当然是调用addAll，这里由于最终要返回List，所以A和R是一个类型，都是List所以才调用addAll） Set characteristics()会返回一个不可变的Characteristics集合，它定义了收集器的行为——尤其是关于流是否可以并行归约，以及可以使用哪些优化的提示，toList这里只用了Characteristics.IDENTITY_FINISH 2.4 并行数据处理并行流：可以通过对收集源调用parallelStream方法来把集合转换为并行流。并行流就是一个把内容分成多个数据块，并用不同的线程分别处理每个数据块的流。 问题:并行流用的线程是从哪儿来的？有多少个？怎么自定义这个过程呢？并行流内部使用了默认的ForkJoinPool，它默认的线程数量就是你的处理器数量，这个值是由Runtime.getRuntime().available-Processors()得到的。但是你可以通过系统属性java.util.concurrent.ForkJoinPool.common.parallelism 来改变线程池大小，如下所示：System.setProperty(“java.util.concurrent.ForkJoinPool.common.parallelism”,”12”);这是一个全局设置，因此它将影响代码中所有的并行流。反过来说，目前还无法专为某个并行流指定这个值。一般而言，让ForkJoinPool的大小等于处理器数量是个不错的默认值，除非你有很好的理由，否则我们强烈建议你不要修改它。 使用并行流：本地测试的过程中，并行流比顺序流效果差。原因可能与机器，处理的数据量，使用并行流的方式等有关系。 是否使用并行流需考虑如下几种情况： 留意装箱。(使用（IntStream、LongStream、DoubleStream来避免装箱拆箱) 有些操作本身在并行流上的性能就比顺序流差。(limit,findFirst等依赖于元素顺序的操作) 考虑流的操作流水线的总计算成本。设N是要处理的元素的总数，Q是一个元素通过流水线的大致处理成本，则N*Q就是这个对成本的一个粗略的定性估计。Q值较高就意味着使用并行流时性能好的可能性比较大。 数据量较小的情况不适合并行流。 考虑流背后的数据结构是否易于分解。 流自身的特点，以及流水线中的中间操作修改流的方式，都可能会改变分解过程的性能。 考虑终端操作中合并步骤的代价是大是小（例如Collector中的combiner方法）。 流的数据源源 | 可分解性—|—ArrayList | 极佳LinkedList | 差IntStream.range | 极佳Stream.iterate | 差HashSet | 好TreeSet | 好 3.Optional使用 使用Optional避免null引用 整洁代码中对null的检查 Optional的使用 java中的null带来了种种问题：典型常见，使代码膨胀，自身无意义等等等。 方法 描述 empty 返回一个空的Optional 实例 filter 如果值存在并且满足提供的谓词，就返回包含该值的Optional 对象；否则返回一个空的Optional 对象 flatMap 如果值存在，就对该值执行提供的mapping函数调用，返回一个Optional 类型的值，否则就返回一个空的Optional 对象 get 如果该值存在，将该值用Optional封装返回，否则抛出一个NoSuchElementException 异常 ifPresent 如果值存在，就执行使用该值的方法调用，否则什么也不做 isPresent 如果值存在就返回true，否则返回false map 如果值存在，就对该值执行提供的mapping 函数调用 of 将指定值用Optional封装之后返回，如果该值为null，则抛出一个NullPointerException异常 ofNullable 将指定值用Optional封装之后返回，如果该值为null，则返回一个空的Optional 对象 orElse 如果有值则将其返回，否则返回一个默认值 orElseGet 如果有值则将其返回，否则返回一个由指定的Supplier接口生成的值 orElseThrow 如果有值则将其返回，否则抛出一个由指定的Supplier接口生成的异常 注意：Optional 无法序列化 4.新的日期和时间API新的 java.time 中包含了所有关于：时钟（Clock）、本地日期（LocalDate）、本地时间（LocalTime）、本地日期时间（LocalDateTime）、时区（ZonedDateTime）和持续时间（Duration）的类。 历史悠久的 Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期 API 大大简化了了日期时间和本地化的管理。目前Java8新增了java.time包定义的类表示日期-时间概念的规则，很方便使用；最重要的一点是值不可变，且线程安全。 本地日期时间API: LocalDate(年月日) LocalTime(时分秒) localDateTime(年月日时分秒) 时区API： ZonedDateTime 时钟API： Clock 计算日期时间差API： Period(处理有关基于时间的日期数量。) Duration(处理有关基于时间的时间量。) 时间格式化API DateTimeFormatter(DateTimeFormatter实例都是线程安全的) 5. 其他 java类库标准base64编码使用方式:`final String text = “测试Abc123!!￥￥”;final String encoded = Base64.getEncoder().encodeToString(text.getBytes(StandardCharsets.UTF_8));System.out.println(encoded); // 5rWL6K+VQWJjMTIzISHvv6Xvv6U=final String decoded = new String(Base64.getDecoder().decode(encoded), StandardCharsets.UTF_8);System.out.println(decoded); // 测试Abc123!!￥￥ // url encodefinal String url = “http://www.jinhui365.com/abc?foo=中文&amp;￥%&amp;bar=hello123&amp;baz=http://abc/def123&quot;;final String encoded2 = Base64.getUrlEncoder().encodeToString(url.getBytes(StandardCharsets.UTF_8));System.out.println(encoded2); // aHR0cDovL3d3dy5qaW5odWkzNjUuY29tL2FiYz9mb2895Lit5paHJu-_pSUmYmFyPWhlbGxvMTIzJmJhej1odHRwOi8vYWJjL2RlZjEyMw==final String decoded2 = new String(Base64.getUrlDecoder().decode(encoded2), StandardCharsets.UTF_8);System.out.println(decoded2); // http://www.jinhui365.com/abc?foo=中文&amp;￥%&amp;bar=hello123&amp;baz=http://abc/def123 ` jvm的变化:PermGen空间被移除了，取而代之的是Metaspace（JEP 122）。 JVM选项 -XX:PermSize与-XX:MaxPermSize分别被-XX:MetaSpaceSize与-XX:MaxMetaspaceSize所代替。 最后Java8 作为 Java 语言的一次重大发布，包含语法上的更改、新的方法与数据类型，以及一些能默默提升应用性能的隐性改善。而且java8有利于提高开发生产力，对于开发者来说是好事，也是趋势。但是生产中使用java8可能存在风险，在正式使用Java8之前，不妨先体验一下java8的神奇。 附《java8 in action》源码:https://github.com/java8/Java8InAction.git]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博图床插件打包]]></title>
    <url>%2F2018%2F04%2F20%2F%E5%BE%AE%E5%8D%9A%E5%9B%BE%E5%BA%8A%E6%8F%92%E4%BB%B6%E6%89%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[前言寻找一个免费的图床软件，微博图床真的是太好不过了~~（手动感谢微博图床！）。之前安装微博图床的chrome插件，本人手贱删了，现在暂时也没翻到墙外，一时间需要重新安装微博图床的chrome插件，甚是着急。网上提供的一些插件也都没用，只能另求他路。 通过自己打包安装微博图床插件 链接到微博图床github地址并下载。url如下： https://github.com/suxiaogang/WeiboPicBed 打开chrome扩展程序打开chrome浏览器，输入如下地址： chrome://extensions/ 打开开发者模式，点击 打包扩展程序 打包扩展程序， 扩展程序目录选择 github上 clone下来的目录，点击打包即可。会在同级目录下生成 .crx 和 .pem结尾文件，其中 .crx结尾文件即是扩展程序 安装微博图床插件直接拖动微博图床插件(WeiboPicBed.crx)至chrome扩展程序即可。然后你就可以肆意上传图片了~]]></content>
      <categories>
        <category>tips</category>
      </categories>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于hexo+nginx快速建站]]></title>
    <url>%2F2018%2F04%2F18%2F%E5%9F%BA%E4%BA%8Ehexo%2Bnginx%E5%BF%AB%E9%80%9F%E5%BB%BA%E7%AB%99%2F</url>
    <content type="text"><![CDATA[前言跟公司大神学习技术知识，发现大神们都有自己的技术博客，拥有个人网站，感觉很酷。所以趁着一波阿里云打折，开启自己个人网站之路。准备着手从个人博客网站开始，无奈自己是一个小小后端程序猿，前端技术了解不多，难道要开始先积累前端技术栈再建站吗？orz。当然不，网上有大量博客框架和教程可以快速建站，比如Hexo,Wordpress等。本人博客 1.0版本就准备用hexo开始。本文并非部署到github上，想要部署到github另行参考。 Hexo特点不妨登录Hexo官网瞧一瞧。 Blazing Fast Markdown Support One-Command Deployment Various Plugins 总之就是很快，支持markdown，简单到一键部署，拥有多样性插件。 快速搭建博客前提条件一台阿里云ECS，环境安装有: Node.js, Git Nginx Ubuntu安装Node.js # 安装依赖包python-software-properties apt-get install python-software-properties curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash - apt-get install nodejs Ubuntu安装Git add-apt-repository ppa:git-core/ppa apt update apt install git Ubuntu安装Nginx（略） 安装 Hexo npm install -g hexo-cli 建站创建博客需要文件 cd /data midir hexo-blog cd hexo-blog hexo init $ npm install 配置网站信息修改 _config.yml 文件, 仅修改了站点信息和地址信息, 更多的配置信息参考官网 # Site title: 陈瑞文的个人网站 subtitle: description: 你好，旅行者 keywords: author: 陈瑞文 language: zh-Hans timezone: # URL ## If your site is put in a subdirectory, set url as &#39;http://yoursite.com/child&#39; and root as &#39;/child/&#39; url: http://www.chenruiwen.cn root: / permalink: :year/:month/:day/:title/ permalink_defaults: 顺便下载了github上比较高排名的hexo主题：Next. git clone https://github.com/iissnan/hexo-theme-next 将文件夹拷贝到Hexo博客目录的themes文件夹下，并修改站点主题 theme: next 剩下的个性化配置参考：http://theme-next.iissnan.com/getting-started.html进行配置。此外，参考了大佬们的文章： 打造个性超赞博客Hexo+NexT+GithubPages的超深度优化 hexo高阶教程next主题优化 hexo的next主题个性化教程:打造炫酷网站 Hexo搭建博客的个性化设置 通过Nginx发布Nginx下载安装略。 部署静态站点生成静态文件: ➜ hexo-blog hexo g ➜ hexo-blog ls _config.yml node_modules public themes db.json package-lock.json scaffolds debug.log package.json source 生成好的静态文件在public 文件夹内。 修改nginx配置文件修改nginx配置文件: vi /usr/local/nginx/conf/nginx.conf 仅需修改http模块的server配置： server { listen 80; server_name www.chenruiwen.cn; charset utf-8; root /data/hexo-blog/public;# 这里是静态文件地址 location / { index index.html; } } 重新加载nginx： nginx -s reload 至此，网站已初步建成。 部署到github上通过git部署 创建GitHub Repository(略) 修改配置文件_config.ymldeploy: type: git repo: https://github.com/crrrrrw/hexo_static_page.git branch: master 安装hexo git插件 npm install hexo-deployer-git --save 部署hexo generate hexo deploy 这样，所有文件就都提交到github库上了。 服务器上，克隆静态文件 cd /data git clone https://github.com/crrrrrw/hexo_static_page.git git pull origin master 即可实现静态文件的更新。然后nginx重新指向这个静态文件目录即可。 自动化部署思路a:脚本定时更新github地址。 思路b:基于git hooks实现push后的自动化部署。 总结总之，搭建博客环境很简单，不过还有很多的工作要做，比如优化自己博客的样式，增加一些有意思的功能等，以后会慢慢优化的。当然，最重要的是写出好的有用的博文分享出去。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装Nginx全记录]]></title>
    <url>%2F2018%2F04%2F16%2Fubuntu%E5%AE%89%E8%A3%85Nginx%E5%85%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言记录下nginx安装过程。本文只演示源码包编译安装。个人比较喜欢自定义安装，灵活。 nginx下载相关官方下载页面：http://nginx.org/en/download.html configure配置文件详解：http://nginx.org/en/docs/configure.html 安装编译工具及库文件安装gcc g++的依赖库 root@chenruiwen:~# apt-get install build-essential root@chenruiwen:~# apt-get install libtool 安装pcre依赖库（http://www.pcre.org/） root@chenruiwen:~# apt-get update root@chenruiwen:~# apt-get install libpcre3 libpcre3-dev 安装zlib依赖库（http://www.zlib.net） root@chenruiwen:~# apt-get install zlib1g-dev 安装SSL依赖库（16.04默认已经安装了） root@chenruiwen:~# apt-get install openssl 安装nginx下载nginx root@chenruiwen:/data/nginx# wget http://nginx.org/download/nginx-1.6.2.tar.gz 解压 root@chenruiwen:/data/nginx# tar xf nginx-1.6.2.tar.gz 进入解压目录 root@chenruiwen:/data/nginx# cd nginx-1.6.2/ 配置 root@chenruiwen:/data/nginx/nginx-1.6.2# ./configure --prefix=/usr/local/nginx 编译,安装 root@chenruiwen:/data/nginx/nginx-1.6.2# make root@chenruiwen:/data/nginx/nginx-1.6.2# make install 测试 root@chenruiwen:/data/nginx/nginx-1.6.2# /usr/local/nginx/sbin/nginx -v nginx version: nginx/1.6.2 nginx安装完成 配置软链接 root@chenruiwen:/data/nginx/nginx-1.6.2# ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx 现在就可以不用路径直接输入nginx启动。 启动 root@chenruiwen:/data/nginx/nginx-1.6.2# nginx -c /usr/local/nginx/conf/nginx.conf Nginx 配置跳转Nginx配置文件详解]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置文件详解]]></title>
    <url>%2F2018%2F04%2F16%2FNginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言参考了网上一些博文和文档，总结了一下，方便日后查阅。 参考文献: http://nginx.org/en/docs/ https://docs.nginx.com/ https://blog.csdn.net/hzsunshine/article/details/63687054 https://blog.csdn.net/field_yang/article/details/52278390 Nginx 工作原理Nginx由内核和模块组成，完成工作是通过查找配置文件将客户端请求映射到一个location block(location是用于URL匹配的命令)，location配置的命令会启动不同模块完成工作。 Nginx模块分为核心模块，基础模块和第三方模块。 核心模块：HTTP模块、EVENT模块(事件)、MAIL模块。 基础模块：HTTP Access模块、HTTP FastCGI模块、HTTP Proxy模块、HTTP Rewrite模块。 第三方模块：HTTP Upstream Request Hash模块、Notice模块、HTTP Access Key模块。 Nginx 配置文件结构配置文件主要由四部分组成：main(全区设置)，server(主机配置)，upstream(负载均衡服务器设置)，和location(URL匹配特定位置设置). main events { .... } http { server { location { ... } location { ... } } server { ... } upstream A { ... } upstream B { ... } } Nignx配置文件详解 全局变量`ruby#Nginx的worker进程运行用户以及用户组，默认为nobody账号user nobody nobody; #Nginx开启的进程数，每个nginx平均耗内存10~12MB，一般指定一个进程就足够了，建议指定和CPU数目相同的进程数即可。worker_processes 1; #worker_processes auto; #以下参数指定了哪个cpu分配给哪个进程，一般来说不用特殊指定。如果一定要设的话，用0和1指定分配方式. #这样设就是给1-4个进程分配单独的核来运行，出现第5个进程是就是随机分配了。eg: #worker_processes 4 #4核CPU #worker_cpu_affinity 0001 0010 0100 1000 #定义全局错误日志定义类型，[debug|info|notice|warn|crit] #error_log logs/error.log info; #指定进程ID存储文件位置 #pid logs/nginx.pid; #一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n的值保持一致。worker_rlimit_nofile用于绑定worker进程和CPU，Linux内核2.4以上使用worker_rlimit_nofile 65535; * 修改ulimit -n 值 #vim /etc/security/limits.conf * soft nproc 65535* hard nproc 65535* soft nofile 65535* hard nofile 655352. 事件配置 event指令用来设置nginx的工作模式及连接数上限。 ```ruby #use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; #use是个事件模块指令，用于指定工作模式，nginx支持的工作模式有select、poll、kqueue、epoll、rtsig、和/dev/poll，其中select和poll都是标准的工作模式。kqueue和epoll是高效的工作模式，其中kqueue用于BSD系统，而epoll用于Linux系统，对于Linux系统，epoll是首选。 use epoll; # worker_connections时间模块指令，用于定义进程的最大连接数，默认为1024,。 #最大客户连接数由worker_connections和worker_processes决定，即max_client=worker_processes*worker_connections,在作为反向代理时变为：max_client= worker_processes*worker_connections/4。 #进程的最大连接数受Linux系统进程的最大打开文件数限制，在执行操作系统命令“ulimit –n 65536”后worker_connections的设置才生效。 worker_connections 65536; #worker工作方式：串行（一定程度降低负载，但服务器吞吐量大时，关闭使用并行方式） #multi_accept on; http参数`http{ #文件扩展名与文件类型映射表,可以减少主配置文件的复杂度。 include mime.types; #默认文件类型,默认为二进制流 default_type application/octet-stream; #nginx的httplog模块指令，用于指定nginx的日志输出格式，main为此日志输出格式的名称，可以在下面access_log指令中使用。 log_format main ‘$remote_addr - $remote_user [$time_local] “$request” ‘ &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;; #连接日志的路径，指定的日志格式放在最后。 #access_log logs/access.log main; #只记录更为严重的错误日志，减少IO压力 error_log logs/error.log crit; #默认编码 #charset utf-8; #服务器名字的hash表大小 server_names_hash_bucket_size 128; #客户端请求单个文件的最大字节数 client_max_body_size 8m; #指定来自客户端请求头的hearerbuffer大小 client_header_buffer_size 32k; #指定客户端请求中较大的消息头的缓存最大数量和大小。 large_client_header_buffers 4 64k; #开启高效传输模式。 sendfile on; #防止网络阻塞 tcp_nopush on; tcp_nodelay on; #设置客户端连续保持活动的超时时间。 keepalive_timeout 65; #keepalive_timeout 0; #设置客户端请求头读取超时时间，超过该时间客户端还没发送任何数据，nginx将返回“request time out（408）”错误。 client_header_timeout 10; #设置客户端请求主体读取超时时间，默认时间为602s，超过该时间客户端还没发送任何数据，nginx将返回“request timeout（408）”错误。 client_body_timeout 10; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 #开启gzip压缩输出 gzip on; #最小压缩文件大小 gzip_min_length 1k; #压缩缓冲区 gzip_buffers 4 16k; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_http_version 1.0; #压缩等级 1-9 等级越高，压缩效果越好，节约宽带，但CPU消耗大 gzip_comp_level 2; #压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn gzip_types text/plain application/x-javascript text/css application/xml; #前端缓存服务器缓存经过压缩的页面 gzip_vary on; } 4. 虚拟主机基本设置 ```python server{ #监听端口 listen 80; #访问域名,多个域名之间用空格隔开 server_name www.example.com; #编码格式，若网页格式与此不同，将被自动转码 #charset koi8-r; #虚拟主机访问日志定义 #access_log logs/host.access.log main; #对URL进行匹配 location / { #访问路径，可相对也可绝对路径 root html; #首页文件。以下按顺序匹配 index index.html index.htm index.jsp; } #错误信息返回页面 #error_page 404 /404.html; # redirect server error pages to the static page /50x.html error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } #php脚本请求全部转发给FastCGI处理 # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} #禁止访问.ht页面 （需ngx_http_access_module模块） # deny access to .htaccess files, if Apache&#39;s document root # concurs with nginx&#39;s one # #location ~ /\.ht { # deny all; #} } #HTTPS虚拟主机定义 # HTTPS server # #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #} 反向代理`#以下配置追加在HTTP的全局变量中 #该部分亦可以使用写进proxy.conf文件中，再使用include指令包含进来，从而使配置显得更简练 #用法：include /opt/nginx/conf/proxy.conf #client_body_buffer_size：用于指定客户端请求主体缓冲区大小，意思就是先保存到本地再传给用户。client_max_body_size 10m;client_body_buffer_size 128k; #nginx跟后端服务器连接超时时间(代理连接超时)proxy_connect_timeout 5; #后端服务器数据回传时间(代理发送超时)proxy_send_timeout 5; #连接成功后，后端服务器响应时间(代理接收超时)proxy_read_timeout 60; #设置代理服务器（nginx）保存用户头信息的缓冲区大小proxy_buffer_size 16k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2）proxy_busy_buffers_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传proxy_temp_file_write_size 64k; #反向代理缓存目录proxy_cache_path /data/proxy/cache levels=1:2 keys_zone=cache_one:500m inactive=1d max_size=1g; #levels=1:2 设置目录深度，第一层目录是1个字符，第2层是2个字符 #keys_zone:设置web缓存名称和内存缓存空间大小 #inactive:自动清除缓存文件时间。 #max_size:硬盘空间最大可使用值。 #指定临时缓存文件的存储路径(路径需和上面路径在同一分区)proxy_temp_path /data/proxy/temp #服务配置server { #侦听的80端口 listen 80; server_name localhost; location / { #反向代理缓存设置命令(proxy_cache zone|off,默认关闭所以要设置) proxy_cache cache_one; #对不同的状态码缓存不同时间 proxy_cache_valid 200 304 12h; #设置以什么样参数获取缓存文件名 proxy_cache_key $host$uri$is_args$args; #后7端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #代理设置 proxy_pass http://IP; #文件过期时间控制 expires 1d; } #配置手动清楚缓存(实现此功能需第三方模块 ngx_cache_purge) #http://www.123.com/2017/0316/17.html访问 #http://www.123.com/purge/2017/0316/17.html清楚URL缓存 location ~ /purge(/.*) { allow 127.0.0.1; deny all; proxy_cache_purge cache_one $host$1$is_args$args; } #设置扩展名以.jsp、.php、.jspx结尾的动态应用程序不做缓存 location ~.*\.(jsp|php|jspx)?$ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://http://IP; } } 6. 负载均衡 upstream是nginx的http upstream模块，这个模块通过一个简单的调度算法来实现客户端IP到后端服务器的负载均衡。 #设置负载均衡器。任意名称命名，使用时调用即可。upstream my_server_pool { #调度算法 #1.轮循（默认）（weight轮循权值） #2.ip_hash：根据每个请求访问IP的hash结果分配。（会话保持） #3.fair:根据后端服务器响应时间最短请求。（upstream_fair模块） #4.url_hash:根据访问的url的hash结果分配。（需hash软件包） #参数： #down：表示不参与负载均衡 #backup:备份服务器 #max_fails:允许最大请求错误次数 #fail_timeout:请求失败后暂停服务时间。 server 192.168.1.109:80 weight=1 max_fails=2 fail_timeout=30; server 192.168.1.108:80 weight=2 max_fails=2 fail_timeout=30; } #负载均衡调用server { … location / { proxy_pass http://my_server_pool; }} 7. URL配置与重写 location支持正则表达式和条件判断匹配，可以通过location指令实现nginx对动静态网页的过滤处理。 ```python # URL配置 #所有扩展名为gif|jpg|jpeg|bmp|png|ico|txt|js|css的静态文件都交给nginx处理 location ~ * \.(gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ { root /web/wwwroot/html; #expires指定静态文件的过期时间 expires 30d; } #将upload和html下的所有文件都交给nginx来处理，当然upload和html目录包含在/web/wwwroot/html目录中 location ~^/(upload|html)/ { root /web/wwwroot/html; expires 30d; } #location对此虚拟机下动态网页的过滤处理，也就是将所有.jsp为后缀的文件都交给本机8080端口处理。 location ~ .*.jsp$ { index index.jsp; proxy_pass http://localhost:8080; } # URL重写 #根据不同的浏览器URL重写 if($http_user_agent ~ Firefox){ rewrite ^(.*)$ /firefox/$1 break; } if($http_user_agent ~ MSIE){ rewrite ^(.*)$ /msie/$1 break; } #实现域名跳转 location / { rewrite ^/(.*)$ https://web8.example.com$1 permanent; } IP限制 #限制IP访问 location / { deny 192.168.0.2； allow 192.168.0.0/24; allow 192.168.1.1; deny all; } Nignx状态监控 #Nginx运行状态，StubStatus模块获取Nginx自启动的工作状态（编译时要开启对应功能） #location /NginxStatus { # #启用StubStatus的工作访问状态 # stub_status on; # #指定StubStaus模块的访问日志文件 # access_log logs/Nginxstatus.log; # #Nginx认证机制（需Apache的htpasswd命令生成） # #auth_basic &quot;NginxStatus&quot;; # #用来认证的密码文件 # #auth_basic_user_file ../htpasswd; #} #nginx的auth_basic采用的是与Apache兼容的密码文件，所以需要采用Apache的htpasswd命令来生成密码文件 #如添加一个fieldyang 用户可以用该方式生成密码文件:/usr/local/apache/bin/htpasswd –c /opt/nginx/conf/htpasswd fieldyang #访问：http://IP/NginxStatus(测试就不加密码验证相关)]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开篇]]></title>
    <url>%2F2018%2F04%2F15%2F%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[2018年，总感觉是个挺有好感的年份，今年也要经历人生的一件大事了，嘿嘿。 过去的一年收获很多，在新公司认识了很多让我敬佩的大牛和可爱的小伙伴们，遇到了让我有些敬畏的领导，严格的要求又让我体验到曾经上学时的感觉。 为什么要建立个人网站？因为coooooooool! 在工作和学习中经常要查询文档和资料，就经常看到某些大牛们的个人网站，博文的质量很高，网站简介又好看，记录下自己走过的坎和分享技术，让我萌生了做个人网站的想法。终于赶上了一次阿里云特价(错过了腾讯云的特价~~)。所以这篇文章就作为我个人网站的开篇了~ hey,hello,world. hello, 2018.]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>杂</tag>
      </tags>
  </entry>
</search>
